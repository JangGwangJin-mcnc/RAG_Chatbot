#!/usr/bin/env python3
"""
bizMOB ì±—ë´‡ - ChromaDB ì „ìš© ë²„ì „
FAISS ì˜ì¡´ì„±ì„ ì™„ì „íˆ ì œê±°í•˜ê³  ChromaDBë§Œ ì‚¬ìš©
"""

import streamlit as st
import os
import sys
import json
import pandas as pd
import re
from datetime import datetime
from typing import List, Dict, Any, Optional
import warnings

# ê²½ê³  ì–µì œ
warnings.filterwarnings("ignore")

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['TORCH_WARN_ON_LOAD'] = '0'
os.environ['TORCH_LOAD_WARN_ONLY'] = '0'
os.environ['PYTORCH_DISABLE_WARNINGS'] = '1'

# ChromaDB ê´€ë ¨ import
try:
    from langchain_community.vectorstores import Chroma
    CHROMADB_AVAILABLE = True
except ImportError:
    st.error("ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install chromadbë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    CHROMADB_AVAILABLE = False

# ê¸°íƒ€ í•„ìš”í•œ importë“¤
try:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.llms import Ollama
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.chains import RetrievalQA
    from langchain.prompts import PromptTemplate
    from langchain_core.documents import Document
    from langchain.retrievers import ParentDocumentRetriever
    from langchain.storage import InMemoryStore
    from langchain.text_splitter import RecursiveCharacterTextSplitter
except ImportError as e:
    st.error(f"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    st.stop()

# íŒŒì¼ ì²˜ë¦¬ ê´€ë ¨ import
try:
    from file_utils import process_file, get_supported_extensions
except ImportError:
    st.error("file_utils.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="bizMOB ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 1rem;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .error-box {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .info-box {
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

def get_chroma_db_path():
    """ChromaDB ê²½ë¡œ ë°˜í™˜"""
    return "./chroma_db"

def get_model_info_path():
    """ëª¨ë¸ ì •ë³´ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    ai_model = st.session_state.get('selected_model', 'llama3.2')
    import re
    safe_model = re.sub(r'[^a-zA-Z0-9_\-]', '_', ai_model)
    return f"vector_db_model_info_{safe_model}.json"

def get_recommended_embedding_model(ai_model_name: str) -> str:
    """AI ëª¨ë¸ì— ë”°ë¥¸ ê¶Œì¥ ì„ë² ë”© ëª¨ë¸ì„ ë°˜í™˜"""
    model_mapping = {
        'llama3.2': 'sentence-transformers/all-mpnet-base-v2',
        'llama3.2:3b': 'sentence-transformers/all-MiniLM-L6-v2',
        'gemma3': 'sentence-transformers/all-mpnet-base-v2',
        'gemma2': 'sentence-transformers/all-MiniLM-L6-v2',
        'mistral': 'sentence-transformers/all-mpnet-base-v2',
        'codellama': 'sentence-transformers/all-mpnet-base-v2'
    }
    
    for key, value in model_mapping.items():
        if key in ai_model_name.lower():
            return value
    return 'sentence-transformers/all-mpnet-base-v2'

def get_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ ë°˜í™˜"""
    selected_embedding = st.session_state.get('selected_embedding_model', 'sentence-transformers/all-mpnet-base-v2')
    return HuggingFaceEmbeddings(model_name=selected_embedding)

def initialize_vector_db():
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    if not CHROMADB_AVAILABLE:
        st.error("ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    try:
        # ChromaDB ë””ë ‰í† ë¦¬ ìƒì„±
        chroma_path = get_chroma_db_path()
        os.makedirs(chroma_path, exist_ok=True)
        
        # ëª¨ë¸ ì •ë³´ ì €ì¥
        model_info = {
            'ai_model': st.session_state.get('selected_model', 'llama3.2'),
            'embedding_model': st.session_state.get('selected_embedding_model', 'sentence-transformers/all-mpnet-base-v2'),
            'timestamp': pd.Timestamp.now().isoformat()
        }
        
        with open(get_model_info_path(), 'w', encoding='utf-8') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)
        
        st.session_state.vector_db_initialized = True
        st.success("âœ… ChromaDB ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        return True
        
    except Exception as e:
        st.error(f"âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

def save_to_chroma_store(documents: list) -> None:
    """ë¬¸ì„œë¥¼ ChromaDBì— ì €ì¥"""
    if not CHROMADB_AVAILABLE:
        st.error("ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    try:
        selected_embedding = st.session_state.get('selected_embedding_model', 'sentence-transformers/all-mpnet-base-v2')
        embeddings = HuggingFaceEmbeddings(model_name=selected_embedding)
        
        st.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {selected_embedding}")
        
        # ChromaDBì— ì €ì¥
        vector_store = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory=get_chroma_db_path()
        )
        vector_store.persist()
        
        st.success("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ (ChromaDB ì‚¬ìš©)")
        
    except Exception as e:
        st.error(f"âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

def load_chroma_store():
    """ChromaDBì—ì„œ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ"""
    if not CHROMADB_AVAILABLE:
        st.error("ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        embeddings = get_embedding_model()
        vector_store = Chroma(
            persist_directory=get_chroma_db_path(),
            embedding_function=embeddings
        )
        return vector_store
    except Exception as e:
        st.error(f"âŒ ChromaDB ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_rag_chain():
    """RAG ì²´ì¸ ìƒì„±"""
    if not CHROMADB_AVAILABLE:
        st.error("ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        # ì„ íƒëœ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
        selected_model = st.session_state.get('selected_model', 'llama3.2')
        
        # Ollama LLM ì´ˆê¸°í™”
        llm = Ollama(model=selected_model)
        
        # ChromaDB ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
        vector_store = load_chroma_store()
        if vector_store is None:
            return None
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        prompt_template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

ì»¨í…ìŠ¤íŠ¸: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # RAG ì²´ì¸ ìƒì„±
        chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vector_store.as_retriever(search_kwargs={"k": 5}),
            chain_type_kwargs={"prompt": prompt}
        )
        
        return chain
        
    except Exception as e:
        st.error(f"âŒ RAG ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def process_question(question: str) -> str:
    """ì§ˆë¬¸ ì²˜ë¦¬"""
    if not CHROMADB_AVAILABLE:
        return "ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    try:
        # RAG ì²´ì¸ ê°€ì ¸ì˜¤ê¸°
        chain = get_rag_chain()
        if chain is None:
            return "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì§ˆë¬¸ ì²˜ë¦¬
        response = chain.invoke({"query": question})
        return response.get("result", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        return f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # session_state ì´ˆê¸°í™”
    if 'selected_model' not in st.session_state:
        st.session_state.selected_model = 'llama3.2'
    if 'selected_embedding_model' not in st.session_state:
        st.session_state.selected_embedding_model = 'sentence-transformers/all-mpnet-base-v2'
    if 'vector_db_initialized' not in st.session_state:
        st.session_state.vector_db_initialized = False
    if 'refresh_vector_db_info' not in st.session_state:
        st.session_state.refresh_vector_db_info = False
    if 'refresh_chroma_viewer' not in st.session_state:
        st.session_state.refresh_chroma_viewer = False
    if 'chroma_viewer_page' not in st.session_state:
        st.session_state.chroma_viewer_page = 1

    # í—¤ë”
    st.markdown('<h1 class="main-header">bizMOB ì±—ë´‡</h1>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">PDF_bizMOB_Guide í´ë”ì˜ bizMOB Platform ê°€ì´ë“œ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.</p>', unsafe_allow_html=True)
    
    # ë™ì ìœ¼ë¡œ AI ëª¨ë¸ëª… ì•ˆë‚´
    ai_model_name = st.session_state.get('selected_model', 'llama3.2')
    if 'llama3.2' in ai_model_name.lower():
        model_display = 'Meta Llama 3.2 ëª¨ë¸'
    else:
        model_display = f"Ollama AI ëª¨ë¸: {ai_model_name}"
    
    st.markdown(f'<p class="sub-header">í˜„ì¬ ì‚¬ìš© ì¤‘: {model_display}</p>', unsafe_allow_html=True)

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # AI ëª¨ë¸ ì„ íƒ
        st.subheader("ğŸ¤– AI ëª¨ë¸ ì„ íƒ")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        try:
            import subprocess
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
            if result.returncode == 0:
                model_lines = result.stdout.strip().split('\n')[1:]  # í—¤ë” ì œì™¸
                model_names = [line.split()[0] for line in model_lines if line.strip()]
            else:
                model_names = ['llama3.2', 'gemma3', 'mistral']
        except:
            model_names = ['llama3.2', 'gemma3', 'mistral']
        
        # ì €ì¥ëœ ëª¨ë¸ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°
        model_info_path = get_model_info_path()
        if os.path.exists(model_info_path):
            try:
                with open(model_info_path, 'r', encoding='utf-8') as f:
                    saved_info = json.load(f)
                    saved_ai_model = saved_info.get('ai_model', 'llama3.2')
                    saved_embedding_model = saved_info.get('embedding_model', 'sentence-transformers/all-mpnet-base-v2')
                
                if saved_ai_model in model_names:
                    st.sidebar.success(f"âœ… ì €ì¥ëœ ëª¨ë¸ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤: {saved_ai_model}")
                else:
                    # ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ llama3.2 ë˜ëŠ” ì²« ë²ˆì§¸ ëª¨ë¸
                    default_index = 0
                    for i, name in enumerate(model_names):
                        if 'llama3.2' in name.lower():
                            default_index = i
                            break
                    saved_ai_model = model_names[default_index]
                    st.session_state.selected_model = model_names[default_index]
            except:
                # ì €ì¥ëœ ì •ë³´ê°€ ì—†ìœ¼ë©´ llama3.2 ë˜ëŠ” ì²« ë²ˆì§¸ ëª¨ë¸
                default_index = 0
                for i, name in enumerate(model_names):
                    if 'llama3.2' in name.lower():
                        default_index = i
                        break
                saved_ai_model = model_names[default_index]
                st.session_state.selected_model = model_names[default_index]
        else:
            # ì €ì¥ëœ ì •ë³´ê°€ ì—†ìœ¼ë©´ llama3.2 ë˜ëŠ” ì²« ë²ˆì§¸ ëª¨ë¸
            default_index = 0
            for i, name in enumerate(model_names):
                if 'llama3.2' in name.lower():
                    default_index = i
                    break
            saved_ai_model = model_names[default_index]
            st.session_state.selected_model = model_names[default_index]
        
        # ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´
        selected_model = st.selectbox(
            "AI ëª¨ë¸ ì„ íƒ",
            model_names,
            index=model_names.index(saved_ai_model) if saved_ai_model in model_names else 0
        )
        
        if selected_model != st.session_state.get('selected_model'):
            st.session_state.selected_model = selected_model
            st.session_state.vector_db_initialized = False
            st.rerun()
        
        # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
        st.subheader("ğŸ” ì„ë² ë”© ëª¨ë¸ ì„ íƒ")
        
        embedding_models = [
            'sentence-transformers/all-mpnet-base-v2',
            'sentence-transformers/all-MiniLM-L6-v2',
            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
        ]
        
        # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ì— ë”°ë¥¸ ê¶Œì¥ ì„ë² ë”© ëª¨ë¸
        current_embedding = get_recommended_embedding_model(selected_model)
        
        # ì €ì¥ëœ ì„ë² ë”© ëª¨ë¸ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°
        if os.path.exists(model_info_path):
            try:
                with open(model_info_path, 'r', encoding='utf-8') as f:
                    saved_info = json.load(f)
                    saved_embedding_model = saved_info.get('embedding_model', current_embedding)
            except:
                saved_embedding_model = current_embedding
        else:
            saved_embedding_model = current_embedding
        
        # ì„ë² ë”© ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´
        selected_embedding = st.selectbox(
            "ì„ë² ë”© ëª¨ë¸ ì„ íƒ",
            embedding_models,
            index=embedding_models.index(saved_embedding_model) if saved_embedding_model in embedding_models else 0
        )
        
        if selected_embedding != st.session_state.get('selected_embedding_model'):
            st.session_state.selected_embedding_model = selected_embedding
            st.session_state.vector_db_initialized = False
            st.rerun()
        
        # ë²¡í„° DB ì´ˆê¸°í™” ë²„íŠ¼
        st.subheader("ğŸ—„ï¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤")
        
        if st.button("ë²¡í„° DB ì´ˆê¸°í™”", type="primary"):
            if initialize_vector_db():
                st.session_state.vector_db_initialized = True
        
        # ë²¡í„° DB ìƒíƒœ í‘œì‹œ
        if st.session_state.get('vector_db_initialized', False):
            st.success("âœ… ë²¡í„° DB ì´ˆê¸°í™”ë¨")
        else:
            st.warning("âš ï¸ ë²¡í„° DB ì´ˆê¸°í™” í•„ìš”")

    # ë©”ì¸ íƒ­
    tab1, tab2, tab3 = st.tabs(["ğŸ’¬ ì±—ë´‡", "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", "â„¹ï¸ ì •ë³´"])
    
    with tab1:
        # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ ì •ë³´ í‘œì‹œ
        selected_model = st.session_state.get('selected_model', 'llama3.2')
        if selected_model:
            # ì €ì¥ëœ ëª¨ë¸ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
            model_info_path = get_model_info_path()
            if os.path.exists(model_info_path):
                try:
                    with open(model_info_path, 'r', encoding='utf-8') as f:
                        model_info = json.load(f)
                        st.info(f"ğŸ“Š í˜„ì¬ ëª¨ë¸: {model_info.get('ai_model', 'Unknown')}")
                        st.info(f"ğŸ” ì„ë² ë”© ëª¨ë¸: {model_info.get('embedding_model', 'Unknown')}")
                        st.info(f"â° ìƒì„± ì‹œê°„: {model_info.get('timestamp', 'Unknown')}")
                except:
                    st.warning("ëª¨ë¸ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.warning("ì €ì¥ëœ ëª¨ë¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
        st.subheader("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°")
        
        # ì§ˆë¬¸ ì…ë ¥
        question = st.text_area("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", height=100)
        
        if st.button("ì§ˆë¬¸í•˜ê¸°", type="primary"):
            if question.strip():
                with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    answer = process_question(question)
                    st.markdown("### ë‹µë³€:")
                    st.write(answer)
            else:
                st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    with tab2:
        st.subheader("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ")
        
        # ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ í‘œì‹œ
        supported_extensions = get_supported_extensions()
        st.info(f"ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹: {', '.join(supported_extensions)}")
        
        # íŒŒì¼ ì—…ë¡œë“œ
        uploaded_files = st.file_uploader(
            "ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=supported_extensions,
            accept_multiple_files=True
        )
        
        if uploaded_files:
            st.write(f"ì—…ë¡œë“œëœ íŒŒì¼: {len(uploaded_files)}ê°œ")
            
            if st.button("ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° DB ì €ì¥", type="primary"):
                with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘..."):
                    all_documents = []
                    
                    for uploaded_file in uploaded_files:
                        try:
                            # íŒŒì¼ ì²˜ë¦¬
                            documents = process_file(uploaded_file)
                            all_documents.extend(documents)
                            st.success(f"âœ… {uploaded_file.name} ì²˜ë¦¬ ì™„ë£Œ")
                        except Exception as e:
                            st.error(f"âŒ {uploaded_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    
                    if all_documents:
                        # ChromaDBì— ì €ì¥
                        save_to_chroma_store(all_documents)
                        st.session_state.vector_db_initialized = True
                    else:
                        st.warning("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab3:
        st.subheader("â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´")
        
        # ChromaDB ìƒíƒœ í™•ì¸
        chroma_path = get_chroma_db_path()
        if os.path.exists(chroma_path):
            st.success("âœ… ChromaDB ë””ë ‰í† ë¦¬ ì¡´ì¬")
            
            # ChromaDB íŒŒì¼ ëª©ë¡
            try:
                chroma_files = os.listdir(chroma_path)
                if chroma_files:
                    st.write("ChromaDB íŒŒì¼:")
                    for file in chroma_files:
                        st.write(f"- {file}")
                else:
                    st.warning("ChromaDBê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"ChromaDB íŒŒì¼ ëª©ë¡ í™•ì¸ ì‹¤íŒ¨: {e}")
        else:
            st.warning("âš ï¸ ChromaDB ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ëª¨ë¸ ì •ë³´ íŒŒì¼ í™•ì¸
        model_info_path = get_model_info_path()
        if os.path.exists(model_info_path):
            st.success("âœ… ëª¨ë¸ ì •ë³´ íŒŒì¼ ì¡´ì¬")
            try:
                with open(model_info_path, 'r', encoding='utf-8') as f:
                    model_info = json.load(f)
                    st.json(model_info)
            except Exception as e:
                st.error(f"ëª¨ë¸ ì •ë³´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        else:
            st.warning("âš ï¸ ëª¨ë¸ ì •ë³´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 