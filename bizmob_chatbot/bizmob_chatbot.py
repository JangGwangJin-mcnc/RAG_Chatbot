#!/usr/bin/env python3
"""
bizMOB ì±—ë´‡ - ChromaDB ì „ìš© ë²„ì „
FAISS ì˜ì¡´ì„±ì„ ì™„ì „íˆ ì œê±°í•˜ê³  ChromaDBë§Œ ì‚¬ìš©
"""

import streamlit as st
import os
import sys
import json
import pandas as pd
import re
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
import warnings

# ê²½ê³  ì–µì œ
warnings.filterwarnings("ignore")

# ë¡œê¹… ì„¤ì •
def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    log_dir = "./logs"
    os.makedirs(log_dir, exist_ok=True)
    
    log_file = os.path.join(log_dir, f"bizmob_chatbot_{datetime.now().strftime('%Y%m%d')}.log")
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ (UTF-8 ì¸ì½”ë”©)
    file_handler = logging.FileHandler(log_file, encoding='utf-8', mode='a')
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(file_formatter)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(console_formatter)
    
    # í•¸ë“¤ëŸ¬ ì¶”ê°€
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ë¡œê±° ì´ˆê¸°í™”
logger = setup_logging()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['TORCH_WARN_ON_LOAD'] = '0'
os.environ['TORCH_LOAD_WARN_ONLY'] = '0'
os.environ['PYTORCH_DISABLE_WARNINGS'] = '1'

# ChromaDB ê´€ë ¨ import
try:
    import chromadb
    from langchain_community.vectorstores import Chroma
    CHROMADB_AVAILABLE = True
except ImportError:
    st.error("ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install chromadbë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    CHROMADB_AVAILABLE = False

# NumPy ê°•ì œ ì„¤ì¹˜ í™•ì¸ ë° ì¬ì„¤ì¹˜
try:
    import numpy
    logger.info(f"NumPy version: {numpy.__version__}")
    
    # NumPyê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
    test_array = numpy.array([1, 2, 3])
    logger.info("NumPy test successful")
    
    # NumPyë¥¼ ì „ì—­ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•¨
    import sys
    sys.modules['numpy'] = numpy
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ìœ¼ë¡œ NumPy í˜¸í™˜ì„± ê°•í™”
    import os
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'
    os.environ['PYTORCH_DISABLE_WARNINGS'] = '1'
    os.environ['TRANSFORMERS_OFFLINE'] = '0'
    
    # PyTorchì™€ NumPy í˜¸í™˜ì„± ê°•ì œ ì„¤ì •
    try:
        import torch
        if hasattr(torch, 'set_default_tensor_type'):
            torch.set_default_tensor_type('torch.FloatTensor')
        logger.info("PyTorch NumPy compatibility set")
    except Exception as e:
        logger.warning(f"PyTorch NumPy compatibility setup failed: {e}")
    
    # Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ NumPy ì„¤ì •
    try:
        import transformers
        if hasattr(transformers, 'np'):
            transformers.np = numpy
        logger.info("Transformers NumPy compatibility set")
    except Exception as e:
        logger.warning(f"Transformers NumPy compatibility setup failed: {e}")
    
    # SentenceTransformersì—ì„œ NumPy ì„¤ì •
    try:
        import sentence_transformers
        if hasattr(sentence_transformers, 'np'):
            sentence_transformers.np = numpy
        logger.info("SentenceTransformers NumPy compatibility set")
    except Exception as e:
        logger.warning(f"SentenceTransformers NumPy compatibility setup failed: {e}")
    
except ImportError:
    logger.error("NumPy is not installed")
    st.error("NumPyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install numpy>=1.26.2ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    st.stop()
except Exception as e:
    logger.error(f"NumPy error: {e}")
    st.error(f"NumPy ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. pip install numpy>=1.26.2ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    st.stop()

# ê¸°íƒ€ í•„ìš”í•œ importë“¤
try:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.llms import Ollama
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.chains import RetrievalQA
    from langchain.prompts import PromptTemplate
    from langchain_core.documents import Document
    from langchain.retrievers import ParentDocumentRetriever
    from langchain.storage import InMemoryStore
    from langchain.text_splitter import RecursiveCharacterTextSplitter
except ImportError as e:
    st.error(f"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    st.stop()

# íŒŒì¼ ì²˜ë¦¬ ê´€ë ¨ import
try:
    from file_utils import process_file, get_supported_extensions
except ImportError:
    st.error("file_utils.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="bizMOB ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 1rem;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .error-box {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .info-box {
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .user-role {
        background-color: #e3f2fd;
        border: 1px solid #bbdefb;
        border-radius: 5px;
        padding: 0.5rem;
        margin: 1rem 0;
        text-align: center;
        font-weight: bold;
    }
    .admin-only {
        background-color: #fff3e0;
        border: 1px solid #ffcc02;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

def check_user_role():
    """ì‚¬ìš©ì ê¶Œí•œ í™•ì¸"""
    # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ì¸ì¦ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ ê´€ë¦¬ì ê¶Œí•œì„ í™•ì¸í•©ë‹ˆë‹¤
    if 'user_role' not in st.session_state:
        # ê¸°ë³¸ê°’ì€ ì¼ë°˜ ì‚¬ìš©ì
        st.session_state.user_role = 'user'
    
    return st.session_state.user_role

def is_admin():
    """ê´€ë¦¬ì ê¶Œí•œ í™•ì¸"""
    return check_user_role() == 'admin'

def show_role_selector():
    """ì‚¬ìš©ì ê¶Œí•œ ì„ íƒê¸°"""
    st.sidebar.subheader("ğŸ‘¤ ì‚¬ìš©ì ê¶Œí•œ")
    
    role_options = {
        'user': 'ì¼ë°˜ ì‚¬ìš©ì',
        'admin': 'ê´€ë¦¬ì'
    }
    
    current_role = st.session_state.get('user_role', 'user')
    selected_role = st.selectbox(
        "ê¶Œí•œ ì„ íƒ",
        options=list(role_options.keys()),
        format_func=lambda x: role_options[x],
        index=0 if current_role == 'user' else 1
    )
    
    if selected_role != current_role:
        st.session_state.user_role = selected_role
        st.rerun()
    
    # í˜„ì¬ ê¶Œí•œ í‘œì‹œ
    role_display = "ê´€ë¦¬ì" if selected_role == 'admin' else "ì¼ë°˜ ì‚¬ìš©ì"
    st.sidebar.markdown(f'<div class="user-role">í˜„ì¬ ê¶Œí•œ: {role_display}</div>', unsafe_allow_html=True)

def get_chroma_db_path():
    """ChromaDB ê²½ë¡œ ë°˜í™˜"""
    return "./chroma_db"

def get_model_info_path():
    """ëª¨ë¸ ì •ë³´ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    ai_model = st.session_state.get('selected_model', 'llama3.2')
    import re
    safe_model = re.sub(r'[^a-zA-Z0-9_\-]', '_', ai_model)
    return f"vector_db_model_info_{safe_model}.json"

def get_recommended_embedding_model(ai_model_name: str) -> str:
    """AI ëª¨ë¸ì— ë”°ë¥¸ ê¶Œì¥ ì„ë² ë”© ëª¨ë¸ì„ ë°˜í™˜"""
    model_mapping = {
        'llama3.2': 'sentence-transformers/all-mpnet-base-v2',
        'llama3.2:3b': 'sentence-transformers/all-MiniLM-L6-v2',
        'gemma3': 'sentence-transformers/all-mpnet-base-v2',
        'gemma2': 'sentence-transformers/all-MiniLM-L6-v2',
        'mistral': 'sentence-transformers/all-mpnet-base-v2',
        'codellama': 'sentence-transformers/all-mpnet-base-v2'
    }
    
    for key, value in model_mapping.items():
        if key in ai_model_name.lower():
            return value
    return 'sentence-transformers/all-mpnet-base-v2'

def get_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ ë°˜í™˜"""
    selected_embedding = st.session_state.get('selected_embedding_model', 'sentence-transformers/all-mpnet-base-v2')
    return HuggingFaceEmbeddings(model_name=selected_embedding)

def initialize_vector_db():
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    logger.info("Vector database initialization started")
    
    if not CHROMADB_AVAILABLE:
        error_msg = "ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        logger.error(error_msg)
        st.error(error_msg)
        return False
    
    try:
        # ChromaDB ë””ë ‰í† ë¦¬ ìƒì„±
        chroma_path = get_chroma_db_path()
        logger.info(f"ChromaDB path: {chroma_path}")
        os.makedirs(chroma_path, exist_ok=True)
        logger.info("ChromaDB directory created successfully")
        
        # ëª¨ë¸ ì •ë³´ ì €ì¥
        model_info = {
            'ai_model': st.session_state.get('selected_model', 'llama3.2'),
            'embedding_model': st.session_state.get('selected_embedding_model', 'sentence-transformers/all-mpnet-base-v2'),
            'timestamp': pd.Timestamp.now().isoformat()
        }
        logger.info(f"Model info: {model_info}")
        
        model_info_path = get_model_info_path()
        logger.info(f"Model info file path: {model_info_path}")
        
        with open(model_info_path, 'w', encoding='utf-8') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)
        
        st.session_state.vector_db_initialized = True
        st.success("âœ… ChromaDB ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info("Vector database initialization successful")
        return True
        
    except Exception as e:
        error_msg = f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"
        logger.error(f"Vector database initialization failed: {e}", exc_info=True)
        st.error(f"âŒ {error_msg}")
        return False

def save_to_chroma_store(documents: list) -> None:
    """ë¬¸ì„œë¥¼ ChromaDBì— ì €ì¥"""
    logger.info(f"Vector database save started - document count: {len(documents)}")
    logger.info(f"Document preview: {[doc.page_content[:50] + '...' if len(doc.page_content) > 50 else doc.page_content for doc in documents[:3]]}")
    
    if not CHROMADB_AVAILABLE:
        error_msg = "ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        logger.error(error_msg)
        st.error(error_msg)
        return
    
    try:
        selected_embedding = st.session_state.get('selected_embedding_model', 'sentence-transformers/all-mpnet-base-v2')
        logger.info(f"Embedding model loading started: {selected_embedding}")
        
        # NumPy ì¬í™•ì¸ ë° ê°•ì œ ì¬ì„¤ì¹˜ ì•ˆë‚´
        try:
            import numpy
            logger.info(f"NumPy recheck - version: {numpy.__version__}")
            
            # NumPy ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            test_array = numpy.array([1, 2, 3])
            test_result = numpy.sum(test_array)
            logger.info(f"NumPy function test successful - result: {test_result}")
            
        except ImportError:
            error_msg = "NumPyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: pip install numpy>=1.26.2"
            logger.error(error_msg)
            st.error(f"âŒ {error_msg}")
            return
        except Exception as e:
            error_msg = f"NumPy ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}. í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: pip uninstall numpy && pip install numpy>=1.26.2"
            logger.error(error_msg)
            st.error(f"âŒ {error_msg}")
            return
        
        # HuggingFaceEmbeddings ì´ˆê¸°í™” ì „ì— NumPy ê°•ì œ ì„¤ì •
        try:
            import numpy as np
            import sys
            import os
            
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ìœ¼ë¡œ NumPy í˜¸í™˜ì„± ê°•í™”
            os.environ['TOKENIZERS_PARALLELISM'] = 'false'
            os.environ['PYTORCH_DISABLE_WARNINGS'] = '1'
            os.environ['TRANSFORMERS_OFFLINE'] = '0'
            
            # ëª¨ë“  ê´€ë ¨ ëª¨ë“ˆì—ì„œ NumPy ê°•ì œ ì„¤ì •
            sys.modules['numpy'] = np
            
            # SentenceTransformersì—ì„œ NumPy ê°•ì œ ì„¤ì •
            try:
                import sentence_transformers
                sentence_transformers.np = np
                logger.info("SentenceTransformers NumPy pre-override successful")
            except Exception as e:
                logger.warning(f"SentenceTransformers NumPy pre-override failed: {e}")
            
            # PyTorchì—ì„œ NumPy í˜¸í™˜ì„± ê°•í™”
            try:
                import torch
                if hasattr(torch, 'set_default_tensor_type'):
                    torch.set_default_tensor_type('torch.FloatTensor')
                logger.info("PyTorch NumPy compatibility set")
            except Exception as e:
                logger.warning(f"PyTorch NumPy compatibility setup failed: {e}")
            
            # HuggingFaceEmbeddings ì´ˆê¸°í™” ì „ì— ëª¨ë“  ëª¨ë“ˆì—ì„œ NumPy ì„¤ì •
            try:
                import transformers
                if hasattr(transformers, 'np'):
                    transformers.np = np
                logger.info("Transformers NumPy override successful")
            except Exception as e:
                logger.warning(f"Transformers NumPy override failed: {e}")
            
            # HuggingFaceEmbeddings ì´ˆê¸°í™”
            embeddings = HuggingFaceEmbeddings(model_name=selected_embedding)
            logger.info("Embedding model loading completed")
            
            st.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {selected_embedding}")
        except Exception as e:
            logger.error(f"Embedding model loading failed: {e}")
            st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return
        
        # ChromaDBì— ì €ì¥
        logger.info("ChromaDB document save started")
        try:
            # NumPy ê°•ì œ ì¬ì„¤ì • ë° í˜¸í™˜ì„± í•´ê²°
            import numpy as np
            import torch
            
            # PyTorchì—ì„œ NumPy ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
            if hasattr(torch, 'set_default_tensor_type'):
                torch.set_default_tensor_type('torch.FloatTensor')
            
            # NumPy ë°°ì—´ í…ŒìŠ¤íŠ¸
            test_embeddings = np.array([[1.0, 2.0, 3.0]])
            logger.info(f"NumPy array test successful - shape: {test_embeddings.shape}")
            
            # PyTorch í…ì„œë¥¼ NumPyë¡œ ë³€í™˜ í…ŒìŠ¤íŠ¸
            torch_tensor = torch.tensor([[1.0, 2.0, 3.0]])
            numpy_array = torch_tensor.numpy()
            logger.info(f"PyTorch -> NumPy conversion test successful - shape: {numpy_array.shape}")
            
            # SentenceTransformersì—ì„œ NumPy ì‚¬ìš© ê°•ì œ ì„¤ì •
            import os
            os.environ['TOKENIZERS_PARALLELISM'] = 'false'
            os.environ['PYTORCH_DISABLE_WARNINGS'] = '1'
            
            # NumPyë¥¼ ì „ì—­ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ SentenceTransformersì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•¨
            import sys
            sys.modules['numpy'] = np
            
            # SentenceTransformers ëª¨ë“ˆì—ì„œ NumPy ì¬ì„¤ì •
            try:
                import sentence_transformers
                if hasattr(sentence_transformers, 'np'):
                    sentence_transformers.np = np
                logger.info("SentenceTransformers NumPy override successful")
            except Exception as e:
                logger.warning(f"SentenceTransformers NumPy override failed: {e}")
            
            # ChromaDBì—ì„œ NumPy ì‚¬ìš© ê°•ì œ ì„¤ì •
            try:
                import chromadb
                if hasattr(chromadb, 'np'):
                    chromadb.np = np
                # ChromaDB ë‚´ë¶€ ëª¨ë“ˆë“¤ì—ë„ NumPy ì„¤ì •
                try:
                    import chromadb.api
                    chromadb.api.np = np
                except:
                    pass
                try:
                    import chromadb.config
                    chromadb.config.np = np
                except:
                    pass
                logger.info("ChromaDB NumPy override successful")
            except Exception as e:
                logger.warning(f"ChromaDB NumPy override failed: {e}")
            
            # LangChainì—ì„œ NumPy ì‚¬ìš© ê°•ì œ ì„¤ì •
            try:
                import langchain_community
                if hasattr(langchain_community, 'np'):
                    langchain_community.np = np
                # LangChain ë‚´ë¶€ ëª¨ë“ˆë“¤ì—ë„ NumPy ì„¤ì •
                try:
                    import langchain_community.vectorstores
                    langchain_community.vectorstores.np = np
                except:
                    pass
                logger.info("LangChain NumPy override successful")
            except Exception as e:
                logger.warning(f"LangChain NumPy override failed: {e}")
            
            # ëª¨ë“  ëª¨ë“ˆì—ì„œ NumPy ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ê°•ì œ ì„¤ì •
            import builtins
            if not hasattr(builtins, '_numpy_original'):
                builtins._numpy_original = builtins.__dict__.get('numpy', None)
                builtins.numpy = np
                logger.info("Builtins NumPy override successful")
            
            # ChromaDB ì´ˆê¸°í™” ì „ì— ì¶”ê°€ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            import os
            os.environ['CHROMA_DB_IMPL'] = 'duckdb+parquet'
            os.environ['CHROMA_SERVER_HOST'] = 'localhost'
            os.environ['CHROMA_SERVER_HTTP_PORT'] = '8000'
            
            # ChromaDB ì €ì¥ ì „ì— ìµœì¢… NumPy í™•ì¸
            try:
                import numpy as final_np
                logger.info(f"Final NumPy check - version: {final_np.__version__}")
                
                # ìƒˆë¡œìš´ ChromaDB í´ë¼ì´ì–¸íŠ¸ ë°©ì‹ ì‚¬ìš©
                chroma_path = get_chroma_db_path()
                
                # ìƒˆë¡œìš´ ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                client = chromadb.Client()
                
                # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
                collection_name = "bizmob_documents"
                try:
                    collection = client.get_collection(name=collection_name)
                    logger.info("Existing collection found")
                except:
                    collection = client.create_collection(name=collection_name)
                    logger.info("New collection created")
                
                # ë¬¸ì„œë¥¼ ChromaDBì— ì €ì¥
                documents_texts = [doc.page_content for doc in documents]
                documents_metadatas = [doc.metadata for doc in documents]
                documents_ids = [f"doc_{i}" for i in range(len(documents))]
                
                # ì„ë² ë”© ìƒì„±
                embeddings_list = embeddings.embed_documents(documents_texts)
                
                # ChromaDBì— ì¶”ê°€
                collection.add(
                    documents=documents_texts,
                    embeddings=embeddings_list,
                    metadatas=documents_metadatas,
                    ids=documents_ids
                )
                
                logger.info("ChromaDB document save completed")
                st.success("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ (ChromaDB ì‚¬ìš©)")
                logger.info("Vector database save successful")
                
            except RuntimeError as e:
                if "Numpy is not available" in str(e):
                    # ë§ˆì§€ë§‰ ì‹œë„: ChromaDBë¥¼ ì§ì ‘ ì´ˆê¸°í™”
                    try:
                        import chromadb
                        client = chromadb.Client()
                        collection = client.create_collection(name="bizmob_documents")
                        
                        # ë¬¸ì„œë¥¼ ì§ì ‘ ì¶”ê°€
                        for i, doc in enumerate(documents):
                            collection.add(
                                documents=[doc.page_content],
                                metadatas=[doc.metadata],
                                ids=[f"doc_{i}"]
                            )
                        
                        logger.info("ChromaDB direct save completed")
                        st.success("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ (ì§ì ‘ ì €ì¥)")
                        return
                    except Exception as direct_error:
                        error_msg = f"ì§ì ‘ ì €ì¥ë„ ì‹¤íŒ¨: {direct_error}. í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: pip uninstall numpy torch sentence-transformers && pip install numpy>=1.26.2 torch>=2.0.0 sentence-transformers>=2.2.0"
                        logger.error(error_msg)
                        st.error(f"âŒ {error_msg}")
                        return
                else:
                    raise e
        except RuntimeError as e:
            if "Numpy is not available" in str(e):
                error_msg = "NumPy ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: pip uninstall numpy torch sentence-transformers && pip install numpy>=1.26.2 torch>=2.0.0 sentence-transformers>=2.2.0"
                logger.error(error_msg)
                st.error(f"âŒ {error_msg}")
                st.info("ğŸ’¡ íŒ: ê°€ìƒí™˜ê²½ì„ ì‚¬ìš© ì¤‘ì´ë¼ë©´ ê°€ìƒí™˜ê²½ì„ ë¹„í™œì„±í™”í•˜ê³  ë‹¤ì‹œ í™œì„±í™”í•œ í›„ ì„¤ì¹˜í•´ë³´ì„¸ìš”.")
            else:
                raise e
        
    except Exception as e:
        error_msg = f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}"
        logger.error(f"Vector database save failed: {e}", exc_info=True)
        st.error(f"âŒ {error_msg}")

def load_chroma_store():
    """ChromaDBì—ì„œ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ"""
    logger.info("ChromaDB vector store loading started")
    
    if not CHROMADB_AVAILABLE:
        error_msg = "ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        logger.error(error_msg)
        st.error(error_msg)
        return None
    
    try:
        chroma_path = get_chroma_db_path()
        logger.info(f"ChromaDB path: {chroma_path}")
        
        # ChromaDB ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
        if not os.path.exists(chroma_path):
            error_msg = f"ChromaDB ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {chroma_path}"
            logger.error(error_msg)
            st.error(f"âŒ {error_msg}")
            return None
        
        logger.info("Embedding model loading started")
        embeddings = get_embedding_model()
        logger.info("Embedding model loading completed")
        
        # ìƒˆë¡œìš´ ChromaDB í´ë¼ì´ì–¸íŠ¸ ë°©ì‹ ì‚¬ìš©
        client = chromadb.Client()
        
        # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
        collection_name = "bizmob_documents"
        try:
            collection = client.get_collection(name=collection_name)
            logger.info("Existing collection found")
        except:
            logger.warning("Collection not found, creating new one")
            collection = client.create_collection(name=collection_name)
        
        # LangChain Chroma ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vector_store = Chroma(
            client=client,
            collection_name=collection_name,
            embedding_function=embeddings
        )
        logger.info("ChromaDB vector store creation completed")
        
        # ë²¡í„° ìŠ¤í† ì–´ ì •ë³´ ë¡œê¹…
        try:
            collection_count = collection.count()
            logger.info(f"ChromaDB collection document count: {collection_count}")
        except Exception as e:
            logger.warning(f"Collection info check failed: {e}")
        
        return vector_store
    except Exception as e:
        error_msg = f"ChromaDB ë¡œë“œ ì‹¤íŒ¨: {e}"
        logger.error(f"ChromaDB load failed: {e}", exc_info=True)
        st.error(f"âŒ {error_msg}")
        return None

def get_rag_chain():
    """RAG ì²´ì¸ ìƒì„±"""
    logger.info("RAG chain creation started")
    
    if not CHROMADB_AVAILABLE:
        error_msg = "ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        logger.error(error_msg)
        st.error(error_msg)
        return None
    
    try:
        # ì„ íƒëœ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
        selected_model = st.session_state.get('selected_model', 'llama3.2')
        logger.info(f"Selected AI model: {selected_model}")
        
        # Ollama LLM ì´ˆê¸°í™”
        logger.info("Ollama LLM initialization started")
        llm = Ollama(model=selected_model)
        logger.info("Ollama LLM initialization completed")
        
        # ChromaDB ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
        logger.info("ChromaDB vector store loading started")
        vector_store = load_chroma_store()
        if vector_store is None:
            error_msg = "ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨"
            logger.error(error_msg)
            return None
        logger.info("ChromaDB vector store loading completed")
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        logger.info("Prompt template creation")
        prompt_template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

ì»¨í…ìŠ¤íŠ¸: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # RAG ì²´ì¸ ìƒì„±
        logger.info("RAG chain creation started")
        chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vector_store.as_retriever(search_kwargs={"k": 5}),
            chain_type_kwargs={"prompt": prompt}
        )
        logger.info("RAG chain creation completed")
        
        return chain
        
    except Exception as e:
        error_msg = f"RAG ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {e}"
        logger.error(f"RAG chain creation failed: {e}", exc_info=True)
        st.error(f"âŒ {error_msg}")
        return None

def process_question(question: str) -> str:
    """ì§ˆë¬¸ ì²˜ë¦¬"""
    logger.info(f"Question processing started: {question[:50]}...")
    
    if not CHROMADB_AVAILABLE:
        error_msg = "ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        logger.error(error_msg)
        return error_msg
    
    try:
        # RAG ì²´ì¸ ê°€ì ¸ì˜¤ê¸°
        logger.info("RAG chain retrieval started")
        chain = get_rag_chain()
        if chain is None:
            error_msg = "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            logger.error(error_msg)
            return error_msg
        logger.info("RAG chain retrieval completed")
        
        # ì§ˆë¬¸ ì²˜ë¦¬
        logger.info("Question processing execution started")
        response = chain.invoke({"query": question})
        result = response.get("result", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        logger.info(f"Question processing completed - answer length: {len(result)}")
        return result
        
    except Exception as e:
        error_msg = f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logger.error(f"Question processing error occurred: {e}", exc_info=True)
        return error_msg

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # session_state ì´ˆê¸°í™”
    if 'selected_model' not in st.session_state:
        st.session_state.selected_model = 'llama3.2'
    if 'selected_embedding_model' not in st.session_state:
        st.session_state.selected_embedding_model = 'sentence-transformers/all-mpnet-base-v2'
    if 'vector_db_initialized' not in st.session_state:
        st.session_state.vector_db_initialized = False
    if 'refresh_vector_db_info' not in st.session_state:
        st.session_state.refresh_vector_db_info = False
    if 'refresh_chroma_viewer' not in st.session_state:
        st.session_state.refresh_chroma_viewer = False
    if 'chroma_viewer_page' not in st.session_state:
        st.session_state.chroma_viewer_page = 1

    # í—¤ë”
    st.markdown('<h1 class="main-header">bizMOB ì±—ë´‡</h1>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">PDF_bizMOB_Guide í´ë”ì˜ bizMOB Platform ê°€ì´ë“œ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.</p>', unsafe_allow_html=True)
    
    # ë™ì ìœ¼ë¡œ AI ëª¨ë¸ëª… ì•ˆë‚´
    ai_model_name = st.session_state.get('selected_model', 'llama3.2')
    if 'llama3.2' in ai_model_name.lower():
        model_display = 'Meta Llama 3.2 ëª¨ë¸'
    else:
        model_display = f"Ollama AI ëª¨ë¸: {ai_model_name}"
    
    st.markdown(f'<p class="sub-header">í˜„ì¬ ì‚¬ìš© ì¤‘: {model_display}</p>', unsafe_allow_html=True)

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # ì‚¬ìš©ì ê¶Œí•œ ì„ íƒê¸°
        show_role_selector()

        # AI ëª¨ë¸ ì„ íƒ
        st.subheader("ğŸ¤– AI ëª¨ë¸ ì„ íƒ")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        try:
            import subprocess
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
            if result.returncode == 0:
                model_lines = result.stdout.strip().split('\n')[1:]  # í—¤ë” ì œì™¸
                model_names = [line.split()[0] for line in model_lines if line.strip()]
            else:
                model_names = ['llama3.2', 'gemma3', 'mistral']
        except:
            model_names = ['llama3.2', 'gemma3', 'mistral']
        
        # model_namesê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        if not model_names:
            model_names = ['llama3.2', 'gemma3', 'mistral']
        
        # ì €ì¥ëœ ëª¨ë¸ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°
        model_info_path = get_model_info_path()
        if os.path.exists(model_info_path):
            try:
                with open(model_info_path, 'r', encoding='utf-8') as f:
                    saved_info = json.load(f)
                    saved_ai_model = saved_info.get('ai_model', 'llama3.2')
                    saved_embedding_model = saved_info.get('embedding_model', 'sentence-transformers/all-mpnet-base-v2')
                
                if saved_ai_model in model_names:
                    st.sidebar.success(f"âœ… ì €ì¥ëœ ëª¨ë¸ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤: {saved_ai_model}")
                else:
                    # ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ llama3.2 ë˜ëŠ” ì²« ë²ˆì§¸ ëª¨ë¸
                    default_index = 0
                    for i, name in enumerate(model_names):
                        if 'llama3.2' in name.lower():
                            default_index = i
                            break
                    saved_ai_model = model_names[default_index]
                    st.session_state.selected_model = model_names[default_index]
            except:
                # ì €ì¥ëœ ì •ë³´ê°€ ì—†ìœ¼ë©´ llama3.2 ë˜ëŠ” ì²« ë²ˆì§¸ ëª¨ë¸
                default_index = 0
                for i, name in enumerate(model_names):
                    if 'llama3.2' in name.lower():
                        default_index = i
                        break
                saved_ai_model = model_names[default_index]
                st.session_state.selected_model = model_names[default_index]
        else:
            # ì €ì¥ëœ ì •ë³´ê°€ ì—†ìœ¼ë©´ llama3.2 ë˜ëŠ” ì²« ë²ˆì§¸ ëª¨ë¸
            default_index = 0
            for i, name in enumerate(model_names):
                if 'llama3.2' in name.lower():
                    default_index = i
                    break
            saved_ai_model = model_names[default_index]
            st.session_state.selected_model = model_names[default_index]
        
        # ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´
        selected_model = st.selectbox(
            "AI ëª¨ë¸ ì„ íƒ",
            model_names,
            index=model_names.index(saved_ai_model) if saved_ai_model in model_names else 0
        )
        
        if selected_model != st.session_state.get('selected_model'):
            st.session_state.selected_model = selected_model
            st.session_state.vector_db_initialized = False
        
        # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
        st.subheader("ğŸ” ì„ë² ë”© ëª¨ë¸ ì„ íƒ")
        
        embedding_models = [
            'sentence-transformers/all-mpnet-base-v2',
            'sentence-transformers/all-MiniLM-L6-v2',
            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
        ]
        
        # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ì— ë”°ë¥¸ ê¶Œì¥ ì„ë² ë”© ëª¨ë¸
        current_embedding = get_recommended_embedding_model(selected_model)
        
        # ì €ì¥ëœ ì„ë² ë”© ëª¨ë¸ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°
        if os.path.exists(model_info_path):
            try:
                with open(model_info_path, 'r', encoding='utf-8') as f:
                    saved_info = json.load(f)
                    saved_embedding_model = saved_info.get('embedding_model', current_embedding)
            except:
                saved_embedding_model = current_embedding
        else:
            saved_embedding_model = current_embedding
        
        # ì„ë² ë”© ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´
        selected_embedding = st.selectbox(
            "ì„ë² ë”© ëª¨ë¸ ì„ íƒ",
            embedding_models,
            index=embedding_models.index(saved_embedding_model) if saved_embedding_model in embedding_models else 0
        )
        
        if selected_embedding != st.session_state.get('selected_embedding_model'):
            st.session_state.selected_embedding_model = selected_embedding
            st.session_state.vector_db_initialized = False
        
        # ë²¡í„° DB ì´ˆê¸°í™” ë²„íŠ¼ (ê´€ë¦¬ìë§Œ)
        if is_admin():
            st.subheader("ğŸ—„ï¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤")
            
            if st.button("ë²¡í„° DB ì´ˆê¸°í™”", type="primary"):
                if initialize_vector_db():
                    st.session_state.vector_db_initialized = True
            
            # ë²¡í„° DB ìƒíƒœ í‘œì‹œ
            if st.session_state.get('vector_db_initialized', False):
                st.success("âœ… ë²¡í„° DB ì´ˆê¸°í™”ë¨")
            else:
                st.warning("âš ï¸ ë²¡í„° DB ì´ˆê¸°í™” í•„ìš”")
        else:
            # ì¼ë°˜ ì‚¬ìš©ìì—ê²ŒëŠ” ê°„ë‹¨í•œ ìƒíƒœë§Œ í‘œì‹œ
            st.subheader("ğŸ—„ï¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤")
            if st.session_state.get('vector_db_initialized', False):
                st.success("âœ… ë²¡í„° DB ì¤€ë¹„ë¨")
            else:
                st.warning("âš ï¸ ê´€ë¦¬ìê°€ ë²¡í„° DBë¥¼ ì´ˆê¸°í™”í•´ì•¼ í•©ë‹ˆë‹¤")

    # ì‚¬ìš©ì ê¶Œí•œì— ë”°ë¥¸ íƒ­ êµ¬ì„±
    user_role = check_user_role()
    
    if user_role == 'user':
        # ì¼ë°˜ ì‚¬ìš©ì: ì±—ë´‡ ê¸°ëŠ¥ë§Œ í‘œì‹œ
        st.markdown('<div class="admin-only">ğŸ”’ ì¼ë°˜ ì‚¬ìš©ì ëª¨ë“œ: ì±—ë´‡ ê¸°ëŠ¥ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.</div>', unsafe_allow_html=True)
        
        # ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
        st.subheader("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°")
        
        # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ ì •ë³´ í‘œì‹œ
        selected_model = st.session_state.get('selected_model', 'llama3.2')
        if selected_model:
            # ì €ì¥ëœ ëª¨ë¸ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
            model_info_path = get_model_info_path()
            if os.path.exists(model_info_path):
                try:
                    with open(model_info_path, 'r', encoding='utf-8') as f:
                        model_info = json.load(f)
                        st.info(f"ğŸ“Š í˜„ì¬ ëª¨ë¸: {model_info.get('ai_model', 'Unknown')}")
                        st.info(f"ğŸ” ì„ë² ë”© ëª¨ë¸: {model_info.get('embedding_model', 'Unknown')}")
                        st.info(f"â° ìƒì„± ì‹œê°„: {model_info.get('timestamp', 'Unknown')}")
                except:
                    st.warning("ëª¨ë¸ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.warning("ì €ì¥ëœ ëª¨ë¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì§ˆë¬¸ ì…ë ¥
        question = st.text_area("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", height=100)
        
        if st.button("ì§ˆë¬¸í•˜ê¸°", type="primary"):
            if question.strip():
                with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    answer = process_question(question)
                    st.markdown("### ë‹µë³€:")
                    st.write(answer)
            else:
                st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    else:
        # ê´€ë¦¬ì: ëª¨ë“  ê¸°ëŠ¥ í‘œì‹œ
        st.markdown('<div class="admin-only">ğŸ”§ ê´€ë¦¬ì ëª¨ë“œ: ëª¨ë“  ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</div>', unsafe_allow_html=True)
        
        # ë©”ì¸ íƒ­
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ’¬ ì±—ë´‡", "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", "ğŸ—„ï¸ ë²¡í„° DB ê´€ë¦¬", "â„¹ï¸ ì •ë³´"])
        
        with tab1:
            # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ ì •ë³´ í‘œì‹œ
            selected_model = st.session_state.get('selected_model', 'llama3.2')
            if selected_model:
                # ì €ì¥ëœ ëª¨ë¸ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
                model_info_path = get_model_info_path()
                if os.path.exists(model_info_path):
                    try:
                        with open(model_info_path, 'r', encoding='utf-8') as f:
                            model_info = json.load(f)
                            st.info(f"ğŸ“Š í˜„ì¬ ëª¨ë¸: {model_info.get('ai_model', 'Unknown')}")
                            st.info(f"ğŸ” ì„ë² ë”© ëª¨ë¸: {model_info.get('embedding_model', 'Unknown')}")
                            st.info(f"â° ìƒì„± ì‹œê°„: {model_info.get('timestamp', 'Unknown')}")
                    except:
                        st.warning("ëª¨ë¸ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.warning("ì €ì¥ëœ ëª¨ë¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
            st.subheader("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°")
            
            # ì§ˆë¬¸ ì…ë ¥
            question = st.text_area("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", height=100)
            
            if st.button("ì§ˆë¬¸í•˜ê¸°", type="primary"):
                if question.strip():
                    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                        answer = process_question(question)
                        st.markdown("### ë‹µë³€:")
                        st.write(answer)
                else:
                    st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        with tab2:
            st.subheader("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ")
            
            # ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ í‘œì‹œ
            supported_extensions = get_supported_extensions()
            st.info(f"ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹: {', '.join(supported_extensions)}")
            
            # íŒŒì¼ ì—…ë¡œë“œ
            uploaded_files = st.file_uploader(
                "ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”",
                type=supported_extensions,
                accept_multiple_files=True
            )
            
            if uploaded_files:
                st.write(f"ì—…ë¡œë“œëœ íŒŒì¼: {len(uploaded_files)}ê°œ")
                
                if st.button("ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° DB ì €ì¥", type="primary"):
                    with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘..."):
                        all_documents = []
                        
                        for uploaded_file in uploaded_files:
                            try:
                                # íŒŒì¼ ì²˜ë¦¬
                                documents = process_file(uploaded_file)
                                all_documents.extend(documents)
                                st.success(f"âœ… {uploaded_file.name} ì²˜ë¦¬ ì™„ë£Œ")
                            except Exception as e:
                                st.error(f"âŒ {uploaded_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        
                        if all_documents:
                            # ChromaDBì— ì €ì¥
                            save_to_chroma_store(all_documents)
                            st.session_state.vector_db_initialized = True
                        else:
                            st.warning("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        with tab3:
            st.subheader("ğŸ—„ï¸ ë²¡í„° DB ê´€ë¦¬")
            
            # ChromaDB ìƒíƒœ í™•ì¸
            chroma_path = get_chroma_db_path()
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("ğŸ“Š ë²¡í„° DB ì •ë³´")
                
                if os.path.exists(chroma_path):
                    try:
                        # ChromaDB íŒŒì¼ ì •ë³´
                        chroma_files = os.listdir(chroma_path)
                        total_size = sum(os.path.getsize(os.path.join(chroma_path, f)) for f in chroma_files if os.path.isfile(os.path.join(chroma_path, f)))
                        
                        st.success("âœ… ChromaDB ì¡´ì¬")
                        st.info(f"ğŸ“ íŒŒì¼ ìˆ˜: {len(chroma_files)}ê°œ")
                        st.info(f"ğŸ’¾ í¬ê¸°: {total_size / 1024:.2f} KB")
                        
                        # íŒŒì¼ ëª©ë¡ í‘œì‹œ
                        with st.expander("ğŸ“‹ íŒŒì¼ ëª©ë¡ ë³´ê¸°"):
                            for file in chroma_files:
                                file_path = os.path.join(chroma_path, file)
                                file_size = os.path.getsize(file_path) / 1024
                                st.write(f"â€¢ {file} ({file_size:.2f} KB)")
                        
                        # ë²¡í„° DB ë‚´ìš© ê²€ìƒ‰
                        st.subheader("ğŸ” ë²¡í„° DB ë‚´ìš© ê²€ìƒ‰")
                        search_query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
                        
                        if search_query and st.button("ê²€ìƒ‰", type="primary"):
                            try:
                                from vector_db_utils import search_chroma_documents
                                results = search_chroma_documents(search_query, st.session_state.get('selected_embedding_model', 'sentence-transformers/all-mpnet-base-v2'))
                                
                                if results:
                                    st.success(f"ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
                                    for i, (doc, score) in enumerate(results):
                                        with st.expander(f"ê²°ê³¼ {i+1} (ìœ ì‚¬ë„: {1/(1+score):.3f})"):
                                            st.write(f"**ë‚´ìš©:** {doc.page_content[:200]}...")
                                            st.write(f"**ë©”íƒ€ë°ì´í„°:** {doc.metadata}")
                                else:
                                    st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                            except Exception as e:
                                st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                        
                    except Exception as e:
                        st.error(f"ChromaDB ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")
                else:
                    st.warning("âš ï¸ ChromaDBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            with col2:
                st.subheader("ğŸ’¾ ë²¡í„° DB ë‹¤ìš´ë¡œë“œ")
                
                if os.path.exists(chroma_path):
                    try:
                        # ChromaDB ì••ì¶• ë‹¤ìš´ë¡œë“œ
                        import zipfile
                        import tempfile
                        
                        if st.button("ğŸ“¦ ChromaDB ì „ì²´ ë‹¤ìš´ë¡œë“œ", type="primary"):
                            with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as tmp_file:
                                with zipfile.ZipFile(tmp_file.name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                                    for root, dirs, files in os.walk(chroma_path):
                                        for file in files:
                                            file_path = os.path.join(root, file)
                                            arcname = os.path.relpath(file_path, chroma_path)
                                            zipf.write(file_path, arcname)
                                
                                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ìƒì„±
                                with open(tmp_file.name, 'rb') as f:
                                    st.download_button(
                                        label="â¬‡ï¸ ë‹¤ìš´ë¡œë“œ",
                                        data=f.read(),
                                        file_name=f"chroma_db_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
                                        mime="application/zip"
                                    )
                                
                                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                                os.unlink(tmp_file.name)
                        
                        # ëª¨ë¸ ì •ë³´ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                        model_info_path = get_model_info_path()
                        if os.path.exists(model_info_path):
                            with open(model_info_path, 'r', encoding='utf-8') as f:
                                model_info_data = f.read()
                            
                            st.download_button(
                                label="ğŸ“„ ëª¨ë¸ ì •ë³´ ë‹¤ìš´ë¡œë“œ",
                                data=model_info_data,
                                file_name=f"model_info_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                                mime="application/json"
                            )
                        
                        # ë²¡í„° DB ì´ˆê¸°í™”
                        st.subheader("ğŸ—‘ï¸ ë²¡í„° DB ì´ˆê¸°í™”")
                        if st.button("âš ï¸ ë²¡í„° DB ì™„ì „ ì‚­ì œ", type="secondary"):
                            try:
                                import shutil
                                shutil.rmtree(chroma_path)
                                st.success("âœ… ë²¡í„° DBê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                                st.session_state.vector_db_initialized = False
                            except Exception as e:
                                st.error(f"ì‚­ì œ ì‹¤íŒ¨: {e}")
                        
                    except Exception as e:
                        st.error(f"ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ ì˜¤ë¥˜: {e}")
                else:
                    st.warning("ë‹¤ìš´ë¡œë“œí•  ChromaDBê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        with tab4:
            st.subheader("â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´")
            
            # ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("ğŸ”§ í™˜ê²½ ì •ë³´")
                st.info(f"Python ë²„ì „: {sys.version}")
                st.info(f"Streamlit ë²„ì „: {st.__version__}")
                
                # ChromaDB ìƒíƒœ í™•ì¸
                chroma_path = get_chroma_db_path()
                if os.path.exists(chroma_path):
                    st.success("âœ… ChromaDB ë””ë ‰í† ë¦¬ ì¡´ì¬")
                    
                    # ChromaDB íŒŒì¼ ëª©ë¡
                    try:
                        chroma_files = os.listdir(chroma_path)
                        if chroma_files:
                            st.write("ChromaDB íŒŒì¼:")
                            for file in chroma_files:
                                st.write(f"- {file}")
                        else:
                            st.warning("ChromaDBê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    except Exception as e:
                        st.error(f"ChromaDB íŒŒì¼ ëª©ë¡ í™•ì¸ ì‹¤íŒ¨: {e}")
                else:
                    st.warning("âš ï¸ ChromaDB ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            with col2:
                st.subheader("ğŸ“‹ ëª¨ë¸ ì •ë³´")
                
                # ëª¨ë¸ ì •ë³´ íŒŒì¼ í™•ì¸
                model_info_path = get_model_info_path()
                if os.path.exists(model_info_path):
                    st.success("âœ… ëª¨ë¸ ì •ë³´ íŒŒì¼ ì¡´ì¬")
                    try:
                        with open(model_info_path, 'r', encoding='utf-8') as f:
                            model_info = json.load(f)
                            st.json(model_info)
                    except Exception as e:
                        st.error(f"ëª¨ë¸ ì •ë³´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
                else:
                    st.warning("âš ï¸ ëª¨ë¸ ì •ë³´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                
                # í˜„ì¬ ì„¸ì…˜ ìƒíƒœ ì •ë³´
                st.subheader("ğŸ›ï¸ í˜„ì¬ ì„¤ì •")
                st.info(f"ì„ íƒëœ AI ëª¨ë¸: {st.session_state.get('selected_model', 'llama3.2')}")
                st.info(f"ì„ íƒëœ ì„ë² ë”© ëª¨ë¸: {st.session_state.get('selected_embedding_model', 'sentence-transformers/all-mpnet-base-v2')}")
                st.info(f"ë²¡í„° DB ì´ˆê¸°í™”: {'âœ… ì™„ë£Œ' if st.session_state.get('vector_db_initialized', False) else 'âš ï¸ í•„ìš”'}")

if __name__ == "__main__":
    main() 