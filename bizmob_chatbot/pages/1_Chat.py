#!/usr/bin/env python3
"""
bizMOB ê°„ë‹¨ ì±„íŒ… í˜ì´ì§€ - RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±
"""

import streamlit as st
import os
import sys
import logging
from datetime import datetime
import subprocess

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

# bizmob_chatbot ëª¨ë“ˆ import
try:
    from bizmob_chatbot import (
        get_cached_rag_chain,
        get_cached_vector_store,
        get_hybrid_retriever,
        process_question,
        get_hybrid_search_config,
        update_hybrid_search_config
    )
    RAG_AVAILABLE = True
except ImportError as e:
    st.error(f"RAG ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    RAG_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    log_dir = "./logs"
    os.makedirs(log_dir, exist_ok=True)
    
    log_file = os.path.join(log_dir, f"chat_page_{datetime.now().strftime('%Y%m%d')}.log")
    
    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler(log_file, encoding='utf-8', mode='a')
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(file_formatter)
    logger.addHandler(console_handler)
    
    return logger

logger = setup_logging()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="bizMOB ì±„íŒ…",
    page_icon="ğŸ’¬",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .main-header {
        text-align: center;
        color: #1f77b4;
        margin-bottom: 2rem;
    }
    .status-box {
        background-color: #e8f5e8;
        border: 1px solid #4caf50;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .error-box {
        background-color: #ffebee;
        border: 1px solid #f44336;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .chat-message-user {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 1rem;
        margin: 0.5rem 0;
        border-bottom-right-radius: 0.3rem;
    }
    .chat-message-assistant {
        background-color: #f3e5f5;
        padding: 1rem;
        border-radius: 1rem;
        margin: 0.5rem 0;
        border-bottom-left-radius: 0.3rem;
    }
    .stButton > button {
        width: 100%;
    }
</style>
""", unsafe_allow_html=True)

def initialize_session():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if 'chat_messages' not in st.session_state:
        st.session_state.chat_messages = []
    if 'selected_model' not in st.session_state:
        st.session_state.selected_model = 'exaone3.5'
    if 'input_counter' not in st.session_state:
        st.session_state.input_counter = 0
    if 'hybrid_search_config' not in st.session_state:
        st.session_state.hybrid_search_config = {'bm25_weight': 0.3, 'vector_weight': 0.7}
    if 'last_retrieved_docs' not in st.session_state:
        st.session_state.last_retrieved_docs = []
    
    # ë²¡í„° ìŠ¤í† ì–´ ê´€ë ¨ ì„¸ì…˜ ìƒíƒœ ìë™ ì´ˆê¸°í™”
    auto_clear_vector_store_cache()

def auto_clear_vector_store_cache():
    """ë²¡í„° ìŠ¤í† ì–´ ê´€ë ¨ ì„¸ì…˜ ìƒíƒœ ìë™ ì •ë¦¬"""
    try:
        logger.info("=== ìë™ ë²¡í„° ìŠ¤í† ì–´ ìºì‹œ ì •ë¦¬ ì‹œì‘ ===")
        
        # ë²¡í„° ìŠ¤í† ì–´ ê´€ë ¨ ìºì‹œ í‚¤ ì°¾ê¸°
        vector_cache_keys = []
        for key in list(st.session_state.keys()):
            if (key.startswith('vector_store_') or 
                key.startswith('global_vector_store_') or
                key.startswith('rag_chain_')):
                vector_cache_keys.append(key)
        
        # ìºì‹œëœ ë²¡í„° ìŠ¤í† ì–´ ìë™ ì œê±°
        for key in vector_cache_keys:
            del st.session_state[key]
            logger.info(f"ìë™ ìºì‹œ ì œê±°: {key}")
        
        # Streamlit ìºì‹œ ìë™ ë¬´íš¨í™”
        try:
            st.cache_resource.clear()
            logger.info("Streamlit ìºì‹œ ìë™ ë¬´íš¨í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"Streamlit ìºì‹œ ìë™ ë¬´íš¨í™” ì‹¤íŒ¨: {e}")
        
        # ì •ë¦¬ëœ ìºì‹œ ìˆ˜ ë¡œê¹…
        if vector_cache_keys:
            logger.info(f"ìë™ ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {len(vector_cache_keys)}ê°œ í‚¤ ì œê±°")
        else:
            logger.info("ì •ë¦¬í•  ë²¡í„° ìŠ¤í† ì–´ ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info("=== ìë™ ë²¡í„° ìŠ¤í† ì–´ ìºì‹œ ì •ë¦¬ ì™„ë£Œ ===")
        
    except Exception as e:
        logger.error(f"ìë™ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ì•± ì‹¤í–‰ì€ ê³„ì†

def add_message(role, content):
    """ì±„íŒ… ë©”ì‹œì§€ ì¶”ê°€"""
    timestamp = datetime.now().strftime("%H:%M")
    st.session_state.chat_messages.append({
        'role': role,
        'content': content,
        'timestamp': timestamp
    })

def display_chat():
    """ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ"""
    for message in st.session_state.chat_messages:
        if message['role'] == 'user':
            st.markdown(f"""
<div class="chat-message-user">
    {message['content']}
    <div style="font-size: 0.7em; opacity: 0.7; margin-top: 5px;">
        {message['timestamp']}
    </div>
</div>
""", unsafe_allow_html=True)
        else:
            st.markdown(f"""
<div class="chat-message-assistant">
    {message['content']}
    <div style="font-size: 0.7em; opacity: 0.7; margin-top: 5px;">
        {message['timestamp']}
    </div>
</div>
""", unsafe_allow_html=True)
            
            # AI ì‘ë‹µì¸ ê²½ìš° ê´€ë ¨ ë¬¸ì„œ ë‚´ìš© í‘œì‹œ
            if message['role'] == 'assistant' and 'last_retrieved_docs' in st.session_state:
                # ë§ˆì§€ë§‰ AI ì‘ë‹µì˜ ë¬¸ì„œì¸ì§€ í™•ì¸
                if len(st.session_state.chat_messages) > 0:
                    last_ai_index = None
                    for i, msg in enumerate(st.session_state.chat_messages):
                        if msg['role'] == 'assistant':
                            last_ai_index = i
                    
                    if last_ai_index == st.session_state.chat_messages.index(message):
                        st.markdown("---")
                        st.markdown("### ğŸ“š **ê´€ë ¨ ë¬¸ì„œ ìƒì„¸ ë‚´ìš©**")
                        st.info("ì•„ë˜ ë¬¸ì„œë¥¼ í´ë¦­í•˜ë©´ ìì„¸í•œ ë‚´ìš©ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        
                        for i, doc in enumerate(st.session_state.last_retrieved_docs):
                            display_document_content(doc, i+1)

def rag_chat_response(user_question):
    """RAG ê¸°ë°˜ ì±„íŒ… ì‘ë‹µ ìƒì„±"""
    logger.info(f"=== RAG ì±„íŒ… ì‘ë‹µ ìƒì„± ì‹œì‘: {user_question[:50]}... ===")
    
    if not RAG_AVAILABLE:
        logger.error("RAG ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. RAG ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    try:
        # í˜„ì¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        current_config = st.session_state.hybrid_search_config
        bm25_weight = current_config['bm25_weight']
        vector_weight = current_config['vector_weight']
        logger.info(f"í˜„ì¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì •: BM25={bm25_weight:.2f}, Vector={vector_weight:.2f}")
        
        # RAG ì„¤ì •ì„ í˜„ì¬ ì„¸ì…˜ ì„¤ì •ìœ¼ë¡œ ì—…ë°ì´íŠ¸ (ë§¤ë²ˆ ê²€ìƒ‰ ì‹œë§ˆë‹¤)
        try:
            config = {
                'bm25_weight': bm25_weight,
                'vector_weight': vector_weight,
                'initial_k': 8,
                'final_k': 3,
                'enable_reranking': True,
                'metadata_boost': True,
                'recency_boost': True
            }
            update_hybrid_search_config(config)
            logger.info(f"RAG í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ: BM25={bm25_weight:.2f}, Vector={vector_weight:.2f}")
        except Exception as e:
            logger.warning(f"RAG ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
        
        # RAG ì²´ì¸ìœ¼ë¡œ ì§ˆë¬¸ ì²˜ë¦¬ (ì•ˆì „í•œ ë²¡í„° ìŠ¤í† ì–´ ì‚¬ìš©)
        logger.info("RAG ì²´ì¸ìœ¼ë¡œ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘...")
        
        # ì•ˆì „í•œ ë²¡í„° ìŠ¤í† ì–´ í™•ì¸
        vector_store = safe_get_vector_store()
        if not vector_store:
            logger.warning("ë²¡í„° ìŠ¤í† ì–´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ RAG ì²´ì¸ ì‚¬ìš©")
        
        result = process_question(user_question)
        
        if result and len(result) == 2:
            response, retrieve_docs = result
            logger.info(f"RAG ì‘ë‹µ ìƒì„± ì„±ê³µ, ê¸¸ì´: {len(response) if response else 0}")
            
            # ê´€ë ¨ ë¬¸ì„œ ì •ë³´ ì¶”ê°€ (í´ë¦­ ê°€ëŠ¥í•œ í˜•íƒœë¡œ)
            if retrieve_docs:
                doc_info = "\n\nğŸ“š **ì°¸ê³  ë¬¸ì„œ:**\n"
                for i, doc in enumerate(retrieve_docs[:3]):
                    source = doc.metadata.get('source', 'Unknown')
                    title = doc.metadata.get('title', 'No Title')
                    relevance = doc.metadata.get('relevance_score', 'N/A')
                    doc_info += f"{i+1}. {title} (ì¶œì²˜: {source}, ê´€ë ¨ì„±: {relevance})\n"
                
                if response:
                    response += doc_info
                else:
                    response = doc_info
                logger.info(f"ì°¸ê³  ë¬¸ì„œ {len(retrieve_docs)}ê°œ ì¶”ê°€ ì™„ë£Œ")
                
                # ë¬¸ì„œ ë‚´ìš©ì„ ì„¸ì…˜ì— ì €ì¥í•˜ì—¬ UIì—ì„œ í‘œì‹œí•  ìˆ˜ ìˆë„ë¡ í•¨
                st.session_state.last_retrieved_docs = retrieve_docs[:3]
            
            if not response:
                response = "ì£„ì†¡í•©ë‹ˆë‹¤. bizMOB Platformì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            return response
        else:
            logger.warning("RAG ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ ì˜ëª»ëœ í˜•ì‹ì…ë‹ˆë‹¤")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. bizMOB Platformì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
    except Exception as e:
        logger.error(f"RAG ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def fallback_chat_response(user_question):
    """í´ë°±: Ollama ì§ì ‘ í˜¸ì¶œ"""
    logger.info(f"=== í´ë°± ì±„íŒ… ì‘ë‹µ ìƒì„±: {user_question[:50]}... ===")
    
    try:
        # bizMOB ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¹ì‹ ì€ bizMOB Platform ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
bizMOB Platformì€ ëª¨ë°”ì¼ ì•± ê°œë°œì„ ìœ„í•œ í”Œë«í¼ìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤:

ì£¼ìš” ê¸°ëŠ¥:
- ëª¨ë°”ì¼ ì•± ê°œë°œ ë° ë°°í¬
- ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì—°ë™
- ì‚¬ìš©ì ê´€ë¦¬ ë° ì¸ì¦
- ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™
- API ê°œë°œ ë° ê´€ë¦¬

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ í•œê¸€ë¡œ í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {user_question}

ë‹µë³€:"""
        
        logger.info(f"í´ë°± í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì™„ë£Œ, ëª¨ë¸: {st.session_state.selected_model}")
        
        # Ollama í˜¸ì¶œ
        logger.info("Ollama í˜¸ì¶œ ì‹œì‘...")
        result = subprocess.run([
            'ollama', 'run', st.session_state.selected_model, prompt
        ], capture_output=True, text=True, timeout=120, encoding='utf-8')
        
        logger.info(f"Ollama í˜¸ì¶œ ì™„ë£Œ, returncode: {result.returncode}")
        
        if result.returncode == 0:
            if result.stdout is not None:
                response = result.stdout.strip()
                if response:
                    logger.info(f"í´ë°± ì‘ë‹µ ìƒì„± ì„±ê³µ, ê¸¸ì´: {len(response)}")
                    return response
                else:
                    logger.warning("í´ë°± ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                    return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            else:
                logger.warning("stdoutì´ Noneì…ë‹ˆë‹¤")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            error_msg = f"ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {result.stderr}"
            logger.error(f"Ollama ì˜¤ë¥˜: {result.stderr}")
            return error_msg
            
    except subprocess.TimeoutExpired:
        logger.error("ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ (2ë¶„)")
        return "ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤ (2ë¶„). ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    except Exception as e:
        logger.error(f"í´ë°± ì‘ë‹µ ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def check_ollama():
    """Ollama ìƒíƒœ í™•ì¸"""
    logger.info("Ollama ìƒíƒœ í™•ì¸ ì‹œì‘")
    
    try:
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        
        if result.returncode == 0:
            logger.info("Ollama ì—°ê²° ì„±ê³µ")
            return True
        else:
            logger.warning(f"Ollama ì—°ê²° ì‹¤íŒ¨, returncode: {result.returncode}")
            return False
    except Exception as e:
        logger.error(f"Ollama ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

def update_rag_config():
    """RAG í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì • ì—…ë°ì´íŠ¸"""
    if RAG_AVAILABLE:
        try:
            current_config = st.session_state.hybrid_search_config
            config = {
                'bm25_weight': current_config['bm25_weight'],
                'vector_weight': current_config['vector_weight'],
                'initial_k': 8,
                'final_k': 3,
                'enable_reranking': True,
                'metadata_boost': True,
                'recency_boost': True
            }
            update_hybrid_search_config(config)
            logger.info(f"RAG ì„¤ì • ìë™ ì—…ë°ì´íŠ¸ ì™„ë£Œ: BM25={current_config['bm25_weight']:.2f}, Vector={current_config['vector_weight']:.2f}")
        except Exception as e:
            logger.warning(f"RAG ì„¤ì • ìë™ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def get_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹œì‘")
    
    try:
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            values = []
            
            for line in lines[1:]:  # ì²« ë²ˆì§¸ ì¤„ì€ í—¤ë”ì´ë¯€ë¡œ ì œì™¸
                if line.strip():
                    parts = line.split()
                    if len(parts) >= 1:
                        model_name = parts[0]
                        values.append(model_name)
            logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ {len(values)}ê°œ ë°œê²¬: {values}")
            return values
        else:
            logger.warning(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨, returncode: {result.returncode}")
            return []
    except Exception as e:
        logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return []

def safe_get_vector_store():
    """ì•ˆì „í•œ ë²¡í„° ìŠ¤í† ì–´ ê°€ì ¸ì˜¤ê¸° (ìë™ ë³µêµ¬ í¬í•¨)"""
    try:
        logger.info("=== ì•ˆì „í•œ ë²¡í„° ìŠ¤í† ì–´ ê°€ì ¸ì˜¤ê¸° ì‹œì‘ ===")
        
        # ë¨¼ì € ìë™ ìºì‹œ ì •ë¦¬ ìˆ˜í–‰
        auto_clear_vector_store_cache()
        
        # ë²¡í„° ìŠ¤í† ì–´ ê°€ì ¸ì˜¤ê¸°
        from bizmob_chatbot import get_cached_vector_store
        vector_store = get_cached_vector_store()
        
        if vector_store:
            logger.info("ë²¡í„° ìŠ¤í† ì–´ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ")
            return vector_store
        else:
            logger.warning("ë²¡í„° ìŠ¤í† ì–´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")
            return None
            
    except Exception as e:
        logger.error(f"ì•ˆì „í•œ ë²¡í„° ìŠ¤í† ì–´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return None

def display_document_content(doc, doc_index):
    """ê°œë³„ ë¬¸ì„œ ë‚´ìš© í‘œì‹œ"""
    with st.expander(f"ğŸ“„ {doc_index}. {doc.metadata.get('title', 'No Title')} (ì¶œì²˜: {doc.metadata.get('source', 'Unknown')})", expanded=False):
        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í‘œì‹œ
        col1, col2 = st.columns(2)
        with col1:
            st.info(f"**ì¶œì²˜:** {doc.metadata.get('source', 'Unknown')}")
        with col2:
            st.info(f"**ê´€ë ¨ì„±:** {doc.metadata.get('relevance_score', 'N/A')}")
        
        # ì¶”ê°€ ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ í‘œì‹œ
        if 'page' in doc.metadata:
            st.info(f"**í˜ì´ì§€:** {doc.metadata['page']}")
        if 'chunk_id' in doc.metadata:
            st.info(f"**ì²­í¬ ID:** {doc.metadata['chunk_id']}")
        
        # ë¬¸ì„œ ë‚´ìš© í‘œì‹œ
        st.markdown("### ğŸ“ ë¬¸ì„œ ë‚´ìš©")
        st.markdown(f"```\n{doc.page_content}\n```")
        
        # ì›ë³¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ (ê°€ëŠ¥í•œ ê²½ìš°)
        source = doc.metadata.get('source', '')
        if source and os.path.exists(source):
            try:
                with open(source, 'rb') as f:
                    file_bytes = f.read()
                    file_name = os.path.basename(source)
                    st.download_button(
                        label=f"ğŸ“¥ {file_name} ë‹¤ìš´ë¡œë“œ",
                        data=file_bytes,
                        file_name=file_name,
                        mime="application/octet-stream"
                    )
            except Exception as e:
                st.warning(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
        else:
            st.warning("ì›ë³¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("=== bizMOB RAG ì±„íŒ… í˜ì´ì§€ ì‹œì‘ ===")
    
    # í˜ì´ì§€ ì§„ì… ì‹œ ìë™ ì„¸ì…˜ ì´ˆê¸°í™”
    logger.info("í˜ì´ì§€ ì§„ì… ì‹œ ìë™ ì„¸ì…˜ ì´ˆê¸°í™” ì‹œì‘")
    auto_clear_vector_store_cache()
    logger.info("í˜ì´ì§€ ì§„ì… ì‹œ ìë™ ì„¸ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
    
    st.markdown('<h1 class="main-header">ğŸ¤– bizMOB Platform RAG ì±—ë´‡</h1>', unsafe_allow_html=True)
    
    # ì„¸ì…˜ ì´ˆê¸°í™”
    logger.info("ì„¸ì…˜ ì´ˆê¸°í™” ì‹œì‘")
    initialize_session()
    logger.info("ì„¸ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ì‚¬ì´ë“œë°” - ì„¤ì • ë° ì •ë³´
    with st.sidebar:
        st.markdown("### âš™ï¸ ì„¤ì •")
        
        # RAG ìƒíƒœ í‘œì‹œ
        if RAG_AVAILABLE:
            st.success("âœ… RAG ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥")
        else:
            st.warning("âš ï¸ RAG ê¸°ëŠ¥ ì‚¬ìš© ë¶ˆê°€")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì •
        st.markdown("#### ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì •")
        
        # BM25 ê°€ì¤‘ì¹˜ ìŠ¬ë¼ì´ë”
        bm25_weight = st.slider(
            "BM25 ê°€ì¤‘ì¹˜",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.hybrid_search_config['bm25_weight'],
            step=0.1,
            key="bm25_weight_slider"
        )
        
        # Vector ê°€ì¤‘ì¹˜ ìë™ ê³„ì‚° (BM25ì™€ í•©ì´ 1.0ì´ ë˜ë„ë¡)
        vector_weight = round(1.0 - bm25_weight, 1)
        
        # Vector ê°€ì¤‘ì¹˜ í‘œì‹œ (ìë™ ë™ê¸°í™”)
        st.info(f"ğŸ”— **ë²¡í„° ê°€ì¤‘ì¹˜: {vector_weight:.1f}** (ìë™ ë™ê¸°í™”)")
        
        # ê°€ì¤‘ì¹˜ í•©ê³„ í‘œì‹œ (í•­ìƒ 1.0ì´ì–´ì•¼ í•¨)
        total_weight = bm25_weight + vector_weight
        st.success(f"âœ… ê°€ì¤‘ì¹˜ í•©ê³„: {total_weight:.1f} (ìë™ ë™ê¸°í™”)")
        
        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        st.session_state.hybrid_search_config = {
            'bm25_weight': bm25_weight,
            'vector_weight': vector_weight
        }
        
        # RAG ì„¤ì • ìë™ ì—…ë°ì´íŠ¸ (ê°€ì¤‘ì¹˜ê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤)
        if RAG_AVAILABLE:
            try:
                config = {
                    'bm25_weight': bm25_weight,
                    'vector_weight': vector_weight,
                    'initial_k': 8,
                    'final_k': 3,
                    'enable_reranking': True,
                    'metadata_boost': True,
                    'recency_boost': True
                }
                update_hybrid_search_config(config)
                st.success(f"âœ… RAG ì„¤ì •ì´ ìë™ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤! (BM25: {bm25_weight:.1f}, Vector: {vector_weight:.1f})")
                logger.info(f"RAG ì„¤ì • ìë™ ì—…ë°ì´íŠ¸ ì™„ë£Œ: BM25={bm25_weight:.2f}, Vector={vector_weight:.2f}")
            except Exception as e:
                st.warning(f"âš ï¸ RAG ì„¤ì • ìë™ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                logger.error(f"RAG ì„¤ì • ìë™ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        else:
            st.success(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! (BM25: {bm25_weight:.1f}, Vector: {vector_weight:.1f})")
        
        # í˜„ì¬ ì„¤ì • ìƒíƒœ í‘œì‹œ
        st.info(f"**í˜„ì¬ ì„¤ì •:** BM25: {bm25_weight:.1f}, Vector: {vector_weight:.1f}")
        
        # ì„¤ì • ë³€ê²½ ì•ˆë‚´
        st.caption("ğŸ’¡ BM25 ìŠ¬ë¼ì´ë”ë¥¼ ì¡°ì •í•˜ë©´ Vector ê°€ì¤‘ì¹˜ê°€ ìë™ìœ¼ë¡œ ë™ê¸°í™”ë˜ê³  RAG ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.")
        
        # ë””ë²„ê¹… ì •ë³´ í‘œì‹œ (ê°œë°œìš©)
        if st.checkbox("ğŸ” ë””ë²„ê¹… ì •ë³´ í‘œì‹œ", key="debug_info"):
            st.json(st.session_state.hybrid_search_config)
            
            # ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸ ë²„íŠ¼
            if st.button("ğŸ§ª ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸", key="test_weights"):
                try:
                    from bizmob_chatbot import test_hybrid_search, get_cached_vector_store
                    vector_store = get_cached_vector_store()
                    if vector_store:
                        test_result = test_hybrid_search(
                            "bizMOB 4.0", 
                            vector_store,
                            bm25_weight=bm25_weight,
                            vector_weight=vector_weight
                        )
                        st.success(f"í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ê²€ìƒ‰ ì‹œê°„: {test_result.get('search_time', 'N/A')}ì´ˆ")
                        st.json(test_result)
                    else:
                        st.warning("ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # ì„¤ì • ì €ì¥ ë²„íŠ¼ (ìˆ˜ë™ ì €ì¥ìš©)
        if st.button("ğŸ’¾ ìˆ˜ë™ ì„¤ì • ì €ì¥", use_container_width=True):
            st.session_state.hybrid_search_config = {
                'bm25_weight': bm25_weight,
                'vector_weight': vector_weight
            }
            
            # RAG ì„¤ì • ì—…ë°ì´íŠ¸
            if RAG_AVAILABLE:
                try:
                    config = {
                        'bm25_weight': bm25_weight,
                        'vector_weight': vector_weight,
                        'initial_k': 8,
                        'final_k': 3,
                        'enable_reranking': True,
                        'metadata_boost': True,
                        'recency_boost': True
                    }
                    update_hybrid_search_config(config)
                    st.success("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                except Exception as e:
                    st.warning(f"RAG ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            else:
                st.success("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        st.markdown("---")
        
        # AI ëª¨ë¸ ì„¤ì •
        st.markdown("#### ğŸ¤– AI ëª¨ë¸ ì„¤ì •")
        
        # Ollama ìƒíƒœ í™•ì¸
        if check_ollama():
            st.success("âœ… Ollama ì—°ê²°ë¨")
            
            # ëª¨ë¸ ì„ íƒ
            available_models = get_available_models()
            if available_models:
                selected_model = st.selectbox(
                    "AI ëª¨ë¸ ì„ íƒ",
                    available_models,
                    index=0 if 'exaone3.5' in available_models else 0
                )
                st.session_state.selected_model = selected_model
                st.info(f"ì„ íƒëœ ëª¨ë¸: {selected_model}")
            else:
                st.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.error("âŒ Ollama ì—°ê²° ì‹¤íŒ¨")
            st.info("Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆê³  ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        st.markdown("---")
        
        # ì±„íŒ… ì •ë³´
        st.markdown("#### ğŸ“Š ì±„íŒ… ì •ë³´")
        st.info(f"ì´ ë©”ì‹œì§€: {len(st.session_state.chat_messages)}ê°œ")
        
        if st.session_state.chat_messages:
            user_count = len([m for m in st.session_state.chat_messages if m['role'] == 'user'])
            assistant_count = len([m for m in st.session_state.chat_messages if m['role'] == 'assistant'])
            
            st.metric("ì‚¬ìš©ì ì§ˆë¬¸", user_count)
            st.metric("AI ë‹µë³€", assistant_count)
        
        # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
        if st.button("ğŸ—‘ï¸ ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.chat_messages = []
            st.rerun()

    st.markdown("---")
    st.markdown("### ğŸ’¡ ì‚¬ìš©ë²•")
    st.markdown("""
    1. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”
    2. 'ì§ˆë¬¸í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
    3. AIê°€ bizMOB Platform ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤
    4. RAG ê¸°ëŠ¥ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤
    """)

    # ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    display_chat()

    # ì§ˆë¬¸ ì…ë ¥
    st.markdown("---")
    user_question = st.text_area(
        "bizMOB Platformì— ëŒ€í•´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”",
        placeholder="bizMOB Platformì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        key=f"input_{st.session_state.input_counter}",
        height=100
    )
            
    # ì§ˆë¬¸ ì œì¶œ ë²„íŠ¼
    if st.button("ì§ˆë¬¸í•˜ê¸°", type="primary", use_container_width=True):
        logger.info("ì§ˆë¬¸í•˜ê¸° ë²„íŠ¼ í´ë¦­ë¨")
        
        if user_question and user_question.strip():
            logger.info(f"ì‚¬ìš©ì ì§ˆë¬¸ ìˆ˜ì‹ : {user_question.strip()}")
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            add_message('user', user_question.strip())
            logger.info("ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ì™„ë£Œ")
            
            # AI ì‘ë‹µ ìƒì„± (RAG ìš°ì„ , í´ë°±ìœ¼ë¡œ Ollama ì§ì ‘ í˜¸ì¶œ)
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                logger.info("AI ì‘ë‹µ ìƒì„± ì‹œì‘")
                
                if RAG_AVAILABLE:
                    logger.info("RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì‹œë„...")
                    response = rag_chat_response(user_question.strip())
                else:
                    logger.info("RAG ì‚¬ìš© ë¶ˆê°€, í´ë°± ì‘ë‹µ ìƒì„±...")
                    response = fallback_chat_response(user_question.strip())
                
                logger.info(f"AI ì‘ë‹µ ìƒì„± ì™„ë£Œ: {response[:100]}...")
                
                add_message('assistant', response)
                logger.info("AI ë©”ì‹œì§€ ì¶”ê°€ ì™„ë£Œ")
            
            # ì…ë ¥ì°½ ì´ˆê¸°í™”
            st.session_state.input_counter += 1
            logger.info("ì…ë ¥ì°½ ì´ˆê¸°í™” ì™„ë£Œ, í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨")
            st.rerun()
        else:
            logger.warning("ë¹ˆ ì§ˆë¬¸ ì…ë ¥ ì‹œë„")
            st.error("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # í•˜ë‹¨ ì•ˆë‚´
    st.markdown("---")
    st.markdown("### ğŸ’¡ ì‚¬ìš© íŒ")
    st.markdown("""
    - bizMOB Platformì— ëŒ€í•œ ì§ˆë¬¸ì„ ììœ ë¡­ê²Œ í•´ë³´ì„¸ìš”
    - RAG ê¸°ëŠ¥ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤
    - êµ¬ì²´ì ì¸ ì§ˆë¬¸ì¼ìˆ˜ë¡ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
    - ì±„íŒ… ê¸°ë¡ì€ ë¸Œë¼ìš°ì € ì„¸ì…˜ ë™ì•ˆ ìœ ì§€ë©ë‹ˆë‹¤
    """)

if __name__ == "__main__":
    main() 