#!/usr/bin/env python3
"""
bizMOB ì±—ë´‡ - ChromaDB ì „ìš© ë²„ì „
FAISS ì˜ì¡´ì„±ì„ ì™„ì „íˆ ì œê±°í•˜ê³  ChromaDBë§Œ ì‚¬ìš©
"""

import streamlit as st
import os
import sys
import json
import pandas as pd
import re
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
import warnings

# ê²½ê³  ì–µì œ
warnings.filterwarnings("ignore")

# ë¡œê¹… ì„¤ì •
def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    log_dir = "./logs"
    os.makedirs(log_dir, exist_ok=True)
    
    log_file = os.path.join(log_dir, f"bizmob_chatbot_{datetime.now().strftime('%Y%m%d')}.log")
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

# ë¡œê±° ì´ˆê¸°í™”
logger = setup_logging()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['TORCH_WARN_ON_LOAD'] = '0'
os.environ['TORCH_LOAD_WARN_ONLY'] = '0'
os.environ['PYTORCH_DISABLE_WARNINGS'] = '1'

# ChromaDB ê´€ë ¨ import
try:
    from langchain_community.vectorstores import Chroma
    CHROMADB_AVAILABLE = True
except ImportError:
    st.error("ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install chromadbë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    CHROMADB_AVAILABLE = False

# NumPy ê°•ì œ ì„¤ì¹˜ í™•ì¸ ë° ì¬ì„¤ì¹˜
try:
    import numpy
    logger.info(f"NumPy ë²„ì „: {numpy.__version__}")
    
    # NumPyê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
    test_array = numpy.array([1, 2, 3])
    logger.info("NumPy í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    
except ImportError:
    logger.error("NumPyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.error("NumPyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install numpy>=1.26.2ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    st.stop()
except Exception as e:
    logger.error(f"NumPy ì˜¤ë¥˜: {e}")
    st.error(f"NumPy ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. pip install numpy>=1.26.2ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    st.stop()

# ê¸°íƒ€ í•„ìš”í•œ importë“¤
try:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.llms import Ollama
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.chains import RetrievalQA
    from langchain.prompts import PromptTemplate
    from langchain_core.documents import Document
    from langchain.retrievers import ParentDocumentRetriever
    from langchain.storage import InMemoryStore
    from langchain.text_splitter import RecursiveCharacterTextSplitter
except ImportError as e:
    st.error(f"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    st.stop()

# íŒŒì¼ ì²˜ë¦¬ ê´€ë ¨ import
try:
    from file_utils import process_file, get_supported_extensions
except ImportError:
    st.error("file_utils.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="bizMOB ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 1rem;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .error-box {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .info-box {
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

def get_chroma_db_path():
    """ChromaDB ê²½ë¡œ ë°˜í™˜"""
    return "./chroma_db"

def get_model_info_path():
    """ëª¨ë¸ ì •ë³´ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    ai_model = st.session_state.get('selected_model', 'llama3.2')
    import re
    safe_model = re.sub(r'[^a-zA-Z0-9_\-]', '_', ai_model)
    return f"vector_db_model_info_{safe_model}.json"

def get_recommended_embedding_model(ai_model_name: str) -> str:
    """AI ëª¨ë¸ì— ë”°ë¥¸ ê¶Œì¥ ì„ë² ë”© ëª¨ë¸ì„ ë°˜í™˜"""
    model_mapping = {
        'llama3.2': 'sentence-transformers/all-mpnet-base-v2',
        'llama3.2:3b': 'sentence-transformers/all-MiniLM-L6-v2',
        'gemma3': 'sentence-transformers/all-mpnet-base-v2',
        'gemma2': 'sentence-transformers/all-MiniLM-L6-v2',
        'mistral': 'sentence-transformers/all-mpnet-base-v2',
        'codellama': 'sentence-transformers/all-mpnet-base-v2'
    }
    
    for key, value in model_mapping.items():
        if key in ai_model_name.lower():
            return value
    return 'sentence-transformers/all-mpnet-base-v2'

def get_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ ë°˜í™˜"""
    selected_embedding = st.session_state.get('selected_embedding_model', 'sentence-transformers/all-mpnet-base-v2')
    return HuggingFaceEmbeddings(model_name=selected_embedding)

def initialize_vector_db():
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹œì‘")
    
    if not CHROMADB_AVAILABLE:
        error_msg = "ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        logger.error(error_msg)
        st.error(error_msg)
        return False
    
    try:
        # ChromaDB ë””ë ‰í† ë¦¬ ìƒì„±
        chroma_path = get_chroma_db_path()
        logger.info(f"ChromaDB ê²½ë¡œ: {chroma_path}")
        os.makedirs(chroma_path, exist_ok=True)
        logger.info("ChromaDB ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ")
        
        # ëª¨ë¸ ì •ë³´ ì €ì¥
        model_info = {
            'ai_model': st.session_state.get('selected_model', 'llama3.2'),
            'embedding_model': st.session_state.get('selected_embedding_model', 'sentence-transformers/all-mpnet-base-v2'),
            'timestamp': pd.Timestamp.now().isoformat()
        }
        logger.info(f"ëª¨ë¸ ì •ë³´: {model_info}")
        
        model_info_path = get_model_info_path()
        logger.info(f"ëª¨ë¸ ì •ë³´ íŒŒì¼ ê²½ë¡œ: {model_info_path}")
        
        with open(model_info_path, 'w', encoding='utf-8') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)
        
        st.session_state.vector_db_initialized = True
        st.success("âœ… ChromaDB ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì„±ê³µ")
        return True
        
    except Exception as e:
        error_msg = f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"
        logger.error(error_msg, exc_info=True)
        st.error(f"âŒ {error_msg}")
        return False

def save_to_chroma_store(documents: list) -> None:
    """ë¬¸ì„œë¥¼ ChromaDBì— ì €ì¥"""
    logger.info(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œì‘ - ë¬¸ì„œ ìˆ˜: {len(documents)}")
    
    if not CHROMADB_AVAILABLE:
        error_msg = "ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        logger.error(error_msg)
        st.error(error_msg)
        return
    
    try:
        selected_embedding = st.session_state.get('selected_embedding_model', 'sentence-transformers/all-mpnet-base-v2')
        logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹œì‘: {selected_embedding}")
        
        # NumPy ì¬í™•ì¸ ë° ê°•ì œ ì¬ì„¤ì¹˜ ì•ˆë‚´
        try:
            import numpy
            logger.info(f"NumPy ì¬í™•ì¸: {numpy.__version__}")
            
            # NumPy ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            test_array = numpy.array([1, 2, 3])
            test_result = numpy.sum(test_array)
            logger.info(f"NumPy ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {test_result}")
            
        except ImportError:
            error_msg = "NumPyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: pip install numpy>=1.26.2"
            logger.error(error_msg)
            st.error(f"âŒ {error_msg}")
            return
        except Exception as e:
            error_msg = f"NumPy ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}. í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: pip uninstall numpy && pip install numpy>=1.26.2"
            logger.error(error_msg)
            st.error(f"âŒ {error_msg}")
            return
        
        embeddings = HuggingFaceEmbeddings(model_name=selected_embedding)
        logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        st.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {selected_embedding}")
        
        # ChromaDBì— ì €ì¥
        logger.info("ChromaDBì— ë¬¸ì„œ ì €ì¥ ì‹œì‘")
        try:
            vector_store = Chroma.from_documents(
                documents=documents,
                embedding=embeddings,
                persist_directory=get_chroma_db_path()
            )
            vector_store.persist()
            logger.info("ChromaDB ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")
            
            st.success("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ (ChromaDB ì‚¬ìš©)")
            logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì„±ê³µ")
        except RuntimeError as e:
            if "Numpy is not available" in str(e):
                error_msg = "NumPy ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: pip uninstall numpy && pip install numpy>=1.26.2"
                logger.error(error_msg)
                st.error(f"âŒ {error_msg}")
                st.info("ğŸ’¡ íŒ: ê°€ìƒí™˜ê²½ì„ ì‚¬ìš© ì¤‘ì´ë¼ë©´ ê°€ìƒí™˜ê²½ì„ ë¹„í™œì„±í™”í•˜ê³  ë‹¤ì‹œ í™œì„±í™”í•œ í›„ ì„¤ì¹˜í•´ë³´ì„¸ìš”.")
            else:
                raise e
        
    except Exception as e:
        error_msg = f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}"
        logger.error(error_msg, exc_info=True)
        st.error(f"âŒ {error_msg}")

def load_chroma_store():
    """ChromaDBì—ì„œ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ"""
    logger.info("ChromaDB ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œì‘")
    
    if not CHROMADB_AVAILABLE:
        error_msg = "ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        logger.error(error_msg)
        st.error(error_msg)
        return None
    
    try:
        chroma_path = get_chroma_db_path()
        logger.info(f"ChromaDB ê²½ë¡œ: {chroma_path}")
        
        # ChromaDB ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
        if not os.path.exists(chroma_path):
            error_msg = f"ChromaDB ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {chroma_path}"
            logger.error(error_msg)
            st.error(f"âŒ {error_msg}")
            return None
        
        logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹œì‘")
        embeddings = get_embedding_model()
        logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        logger.info("ChromaDB ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹œì‘")
        vector_store = Chroma(
            persist_directory=chroma_path,
            embedding_function=embeddings
        )
        logger.info("ChromaDB ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
        
        # ë²¡í„° ìŠ¤í† ì–´ ì •ë³´ ë¡œê¹…
        try:
            collection_count = vector_store._collection.count()
            logger.info(f"ChromaDB ì»¬ë ‰ì…˜ ë¬¸ì„œ ìˆ˜: {collection_count}")
        except Exception as e:
            logger.warning(f"ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        return vector_store
    except Exception as e:
        error_msg = f"ChromaDB ë¡œë“œ ì‹¤íŒ¨: {e}"
        logger.error(error_msg, exc_info=True)
        st.error(f"âŒ {error_msg}")
        return None

def get_rag_chain():
    """RAG ì²´ì¸ ìƒì„±"""
    logger.info("RAG ì²´ì¸ ìƒì„± ì‹œì‘")
    
    if not CHROMADB_AVAILABLE:
        error_msg = "ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        logger.error(error_msg)
        st.error(error_msg)
        return None
    
    try:
        # ì„ íƒëœ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
        selected_model = st.session_state.get('selected_model', 'llama3.2')
        logger.info(f"ì„ íƒëœ AI ëª¨ë¸: {selected_model}")
        
        # Ollama LLM ì´ˆê¸°í™”
        logger.info("Ollama LLM ì´ˆê¸°í™” ì‹œì‘")
        llm = Ollama(model=selected_model)
        logger.info("Ollama LLM ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ChromaDB ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
        logger.info("ChromaDB ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œì‘")
        vector_store = load_chroma_store()
        if vector_store is None:
            error_msg = "ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨"
            logger.error(error_msg)
            return None
        logger.info("ChromaDB ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        logger.info("í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±")
        prompt_template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

ì»¨í…ìŠ¤íŠ¸: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # RAG ì²´ì¸ ìƒì„±
        logger.info("RAG ì²´ì¸ ìƒì„± ì‹œì‘")
        chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vector_store.as_retriever(search_kwargs={"k": 5}),
            chain_type_kwargs={"prompt": prompt}
        )
        logger.info("RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")
        
        return chain
        
    except Exception as e:
        error_msg = f"RAG ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {e}"
        logger.error(error_msg, exc_info=True)
        st.error(f"âŒ {error_msg}")
        return None

def process_question(question: str) -> str:
    """ì§ˆë¬¸ ì²˜ë¦¬"""
    logger.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: {question[:50]}...")
    
    if not CHROMADB_AVAILABLE:
        error_msg = "ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        logger.error(error_msg)
        return error_msg
    
    try:
        # RAG ì²´ì¸ ê°€ì ¸ì˜¤ê¸°
        logger.info("RAG ì²´ì¸ ê°€ì ¸ì˜¤ê¸° ì‹œì‘")
        chain = get_rag_chain()
        if chain is None:
            error_msg = "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            logger.error(error_msg)
            return error_msg
        logger.info("RAG ì²´ì¸ ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ")
        
        # ì§ˆë¬¸ ì²˜ë¦¬
        logger.info("ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤í–‰ ì‹œì‘")
        response = chain.invoke({"query": question})
        result = response.get("result", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        logger.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ - ë‹µë³€ ê¸¸ì´: {len(result)}")
        return result
        
    except Exception as e:
        error_msg = f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logger.error(error_msg, exc_info=True)
        return error_msg

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # session_state ì´ˆê¸°í™”
    if 'selected_model' not in st.session_state:
        st.session_state.selected_model = 'llama3.2'
    if 'selected_embedding_model' not in st.session_state:
        st.session_state.selected_embedding_model = 'sentence-transformers/all-mpnet-base-v2'
    if 'vector_db_initialized' not in st.session_state:
        st.session_state.vector_db_initialized = False
    if 'refresh_vector_db_info' not in st.session_state:
        st.session_state.refresh_vector_db_info = False
    if 'refresh_chroma_viewer' not in st.session_state:
        st.session_state.refresh_chroma_viewer = False
    if 'chroma_viewer_page' not in st.session_state:
        st.session_state.chroma_viewer_page = 1

    # í—¤ë”
    st.markdown('<h1 class="main-header">bizMOB ì±—ë´‡</h1>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">PDF_bizMOB_Guide í´ë”ì˜ bizMOB Platform ê°€ì´ë“œ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.</p>', unsafe_allow_html=True)
    
    # ë™ì ìœ¼ë¡œ AI ëª¨ë¸ëª… ì•ˆë‚´
    ai_model_name = st.session_state.get('selected_model', 'llama3.2')
    if 'llama3.2' in ai_model_name.lower():
        model_display = 'Meta Llama 3.2 ëª¨ë¸'
    else:
        model_display = f"Ollama AI ëª¨ë¸: {ai_model_name}"
    
    st.markdown(f'<p class="sub-header">í˜„ì¬ ì‚¬ìš© ì¤‘: {model_display}</p>', unsafe_allow_html=True)

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # AI ëª¨ë¸ ì„ íƒ
        st.subheader("ğŸ¤– AI ëª¨ë¸ ì„ íƒ")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        try:
            import subprocess
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
            if result.returncode == 0:
                model_lines = result.stdout.strip().split('\n')[1:]  # í—¤ë” ì œì™¸
                model_names = [line.split()[0] for line in model_lines if line.strip()]
            else:
                model_names = ['llama3.2', 'gemma3', 'mistral']
        except:
            model_names = ['llama3.2', 'gemma3', 'mistral']
        
        # model_namesê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        if not model_names:
            model_names = ['llama3.2', 'gemma3', 'mistral']
        
        # ì €ì¥ëœ ëª¨ë¸ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°
        model_info_path = get_model_info_path()
        if os.path.exists(model_info_path):
            try:
                with open(model_info_path, 'r', encoding='utf-8') as f:
                    saved_info = json.load(f)
                    saved_ai_model = saved_info.get('ai_model', 'llama3.2')
                    saved_embedding_model = saved_info.get('embedding_model', 'sentence-transformers/all-mpnet-base-v2')
                
                if saved_ai_model in model_names:
                    st.sidebar.success(f"âœ… ì €ì¥ëœ ëª¨ë¸ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤: {saved_ai_model}")
                else:
                    # ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ llama3.2 ë˜ëŠ” ì²« ë²ˆì§¸ ëª¨ë¸
                    default_index = 0
                    for i, name in enumerate(model_names):
                        if 'llama3.2' in name.lower():
                            default_index = i
                            break
                    saved_ai_model = model_names[default_index]
                    st.session_state.selected_model = model_names[default_index]
            except:
                # ì €ì¥ëœ ì •ë³´ê°€ ì—†ìœ¼ë©´ llama3.2 ë˜ëŠ” ì²« ë²ˆì§¸ ëª¨ë¸
                default_index = 0
                for i, name in enumerate(model_names):
                    if 'llama3.2' in name.lower():
                        default_index = i
                        break
                saved_ai_model = model_names[default_index]
                st.session_state.selected_model = model_names[default_index]
        else:
            # ì €ì¥ëœ ì •ë³´ê°€ ì—†ìœ¼ë©´ llama3.2 ë˜ëŠ” ì²« ë²ˆì§¸ ëª¨ë¸
            default_index = 0
            for i, name in enumerate(model_names):
                if 'llama3.2' in name.lower():
                    default_index = i
                    break
            saved_ai_model = model_names[default_index]
            st.session_state.selected_model = model_names[default_index]
        
        # ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´
        selected_model = st.selectbox(
            "AI ëª¨ë¸ ì„ íƒ",
            model_names,
            index=model_names.index(saved_ai_model) if saved_ai_model in model_names else 0
        )
        
        if selected_model != st.session_state.get('selected_model'):
            st.session_state.selected_model = selected_model
            st.session_state.vector_db_initialized = False
        
        # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
        st.subheader("ğŸ” ì„ë² ë”© ëª¨ë¸ ì„ íƒ")
        
        embedding_models = [
            'sentence-transformers/all-mpnet-base-v2',
            'sentence-transformers/all-MiniLM-L6-v2',
            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
        ]
        
        # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ì— ë”°ë¥¸ ê¶Œì¥ ì„ë² ë”© ëª¨ë¸
        current_embedding = get_recommended_embedding_model(selected_model)
        
        # ì €ì¥ëœ ì„ë² ë”© ëª¨ë¸ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°
        if os.path.exists(model_info_path):
            try:
                with open(model_info_path, 'r', encoding='utf-8') as f:
                    saved_info = json.load(f)
                    saved_embedding_model = saved_info.get('embedding_model', current_embedding)
            except:
                saved_embedding_model = current_embedding
        else:
            saved_embedding_model = current_embedding
        
        # ì„ë² ë”© ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´
        selected_embedding = st.selectbox(
            "ì„ë² ë”© ëª¨ë¸ ì„ íƒ",
            embedding_models,
            index=embedding_models.index(saved_embedding_model) if saved_embedding_model in embedding_models else 0
        )
        
        if selected_embedding != st.session_state.get('selected_embedding_model'):
            st.session_state.selected_embedding_model = selected_embedding
            st.session_state.vector_db_initialized = False
        
        # ë²¡í„° DB ì´ˆê¸°í™” ë²„íŠ¼
        st.subheader("ğŸ—„ï¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤")
        
        if st.button("ë²¡í„° DB ì´ˆê¸°í™”", type="primary"):
            if initialize_vector_db():
                st.session_state.vector_db_initialized = True
        
        # ë²¡í„° DB ìƒíƒœ í‘œì‹œ
        if st.session_state.get('vector_db_initialized', False):
            st.success("âœ… ë²¡í„° DB ì´ˆê¸°í™”ë¨")
        else:
            st.warning("âš ï¸ ë²¡í„° DB ì´ˆê¸°í™” í•„ìš”")

    # ë©”ì¸ íƒ­
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ’¬ ì±—ë´‡", "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", "ğŸ—„ï¸ ë²¡í„° DB ê´€ë¦¬", "â„¹ï¸ ì •ë³´"])
    
    with tab1:
        # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ ì •ë³´ í‘œì‹œ
        selected_model = st.session_state.get('selected_model', 'llama3.2')
        if selected_model:
            # ì €ì¥ëœ ëª¨ë¸ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
            model_info_path = get_model_info_path()
            if os.path.exists(model_info_path):
                try:
                    with open(model_info_path, 'r', encoding='utf-8') as f:
                        model_info = json.load(f)
                        st.info(f"ğŸ“Š í˜„ì¬ ëª¨ë¸: {model_info.get('ai_model', 'Unknown')}")
                        st.info(f"ğŸ” ì„ë² ë”© ëª¨ë¸: {model_info.get('embedding_model', 'Unknown')}")
                        st.info(f"â° ìƒì„± ì‹œê°„: {model_info.get('timestamp', 'Unknown')}")
                except:
                    st.warning("ëª¨ë¸ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.warning("ì €ì¥ëœ ëª¨ë¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
        st.subheader("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°")
        
        # ì§ˆë¬¸ ì…ë ¥
        question = st.text_area("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", height=100)
        
        if st.button("ì§ˆë¬¸í•˜ê¸°", type="primary"):
            if question.strip():
                with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    answer = process_question(question)
                    st.markdown("### ë‹µë³€:")
                    st.write(answer)
            else:
                st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    with tab2:
        st.subheader("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ")
        
        # ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ í‘œì‹œ
        supported_extensions = get_supported_extensions()
        st.info(f"ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹: {', '.join(supported_extensions)}")
        
        # íŒŒì¼ ì—…ë¡œë“œ
        uploaded_files = st.file_uploader(
            "ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=supported_extensions,
            accept_multiple_files=True
        )
        
        if uploaded_files:
            st.write(f"ì—…ë¡œë“œëœ íŒŒì¼: {len(uploaded_files)}ê°œ")
            
            if st.button("ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° DB ì €ì¥", type="primary"):
                with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘..."):
                    all_documents = []
                    
                    for uploaded_file in uploaded_files:
                        try:
                            # íŒŒì¼ ì²˜ë¦¬
                            documents = process_file(uploaded_file)
                            all_documents.extend(documents)
                            st.success(f"âœ… {uploaded_file.name} ì²˜ë¦¬ ì™„ë£Œ")
                        except Exception as e:
                            st.error(f"âŒ {uploaded_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    
                    if all_documents:
                        # ChromaDBì— ì €ì¥
                        save_to_chroma_store(all_documents)
                        st.session_state.vector_db_initialized = True
                    else:
                        st.warning("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab3:
        st.subheader("ğŸ—„ï¸ ë²¡í„° DB ê´€ë¦¬")
        
        # ChromaDB ìƒíƒœ í™•ì¸
        chroma_path = get_chroma_db_path()
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“Š ë²¡í„° DB ì •ë³´")
            
            if os.path.exists(chroma_path):
                try:
                    # ChromaDB íŒŒì¼ ì •ë³´
                    chroma_files = os.listdir(chroma_path)
                    total_size = sum(os.path.getsize(os.path.join(chroma_path, f)) for f in chroma_files if os.path.isfile(os.path.join(chroma_path, f)))
                    
                    st.success("âœ… ChromaDB ì¡´ì¬")
                    st.info(f"ğŸ“ íŒŒì¼ ìˆ˜: {len(chroma_files)}ê°œ")
                    st.info(f"ğŸ’¾ í¬ê¸°: {total_size / 1024:.2f} KB")
                    
                    # íŒŒì¼ ëª©ë¡ í‘œì‹œ
                    with st.expander("ğŸ“‹ íŒŒì¼ ëª©ë¡ ë³´ê¸°"):
                        for file in chroma_files:
                            file_path = os.path.join(chroma_path, file)
                            file_size = os.path.getsize(file_path) / 1024
                            st.write(f"â€¢ {file} ({file_size:.2f} KB)")
                    
                    # ë²¡í„° DB ë‚´ìš© ê²€ìƒ‰
                    st.subheader("ğŸ” ë²¡í„° DB ë‚´ìš© ê²€ìƒ‰")
                    search_query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
                    
                    if search_query and st.button("ê²€ìƒ‰", type="primary"):
                        try:
                            from vector_db_utils import search_chroma_documents
                            results = search_chroma_documents(search_query, st.session_state.get('selected_embedding_model', 'sentence-transformers/all-mpnet-base-v2'))
                            
                            if results:
                                st.success(f"ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
                                for i, (doc, score) in enumerate(results):
                                    with st.expander(f"ê²°ê³¼ {i+1} (ìœ ì‚¬ë„: {1/(1+score):.3f})"):
                                        st.write(f"**ë‚´ìš©:** {doc.page_content[:200]}...")
                                        st.write(f"**ë©”íƒ€ë°ì´í„°:** {doc.metadata}")
                            else:
                                st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        except Exception as e:
                            st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                    
                except Exception as e:
                    st.error(f"ChromaDB ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")
            else:
                st.warning("âš ï¸ ChromaDBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        with col2:
            st.subheader("ğŸ’¾ ë²¡í„° DB ë‹¤ìš´ë¡œë“œ")
            
            if os.path.exists(chroma_path):
                try:
                    # ChromaDB ì••ì¶• ë‹¤ìš´ë¡œë“œ
                    import zipfile
                    import tempfile
                    
                    if st.button("ğŸ“¦ ChromaDB ì „ì²´ ë‹¤ìš´ë¡œë“œ", type="primary"):
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as tmp_file:
                            with zipfile.ZipFile(tmp_file.name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                                for root, dirs, files in os.walk(chroma_path):
                                    for file in files:
                                        file_path = os.path.join(root, file)
                                        arcname = os.path.relpath(file_path, chroma_path)
                                        zipf.write(file_path, arcname)
                            
                            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ìƒì„±
                            with open(tmp_file.name, 'rb') as f:
                                st.download_button(
                                    label="â¬‡ï¸ ë‹¤ìš´ë¡œë“œ",
                                    data=f.read(),
                                    file_name=f"chroma_db_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
                                    mime="application/zip"
                                )
                            
                            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                            os.unlink(tmp_file.name)
                    
                    # ëª¨ë¸ ì •ë³´ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                    model_info_path = get_model_info_path()
                    if os.path.exists(model_info_path):
                        with open(model_info_path, 'r', encoding='utf-8') as f:
                            model_info_data = f.read()
                        
                        st.download_button(
                            label="ğŸ“„ ëª¨ë¸ ì •ë³´ ë‹¤ìš´ë¡œë“œ",
                            data=model_info_data,
                            file_name=f"model_info_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                            mime="application/json"
                        )
                    
                    # ë²¡í„° DB ì´ˆê¸°í™”
                    st.subheader("ğŸ—‘ï¸ ë²¡í„° DB ì´ˆê¸°í™”")
                    if st.button("âš ï¸ ë²¡í„° DB ì™„ì „ ì‚­ì œ", type="secondary"):
                        try:
                            import shutil
                            shutil.rmtree(chroma_path)
                            st.success("âœ… ë²¡í„° DBê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                            st.session_state.vector_db_initialized = False
                        except Exception as e:
                            st.error(f"ì‚­ì œ ì‹¤íŒ¨: {e}")
                    
                except Exception as e:
                    st.error(f"ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ ì˜¤ë¥˜: {e}")
            else:
                st.warning("ë‹¤ìš´ë¡œë“œí•  ChromaDBê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab4:
        st.subheader("â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´")
        
        # ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ”§ í™˜ê²½ ì •ë³´")
            st.info(f"Python ë²„ì „: {sys.version}")
            st.info(f"Streamlit ë²„ì „: {st.__version__}")
            
            # ChromaDB ìƒíƒœ í™•ì¸
            chroma_path = get_chroma_db_path()
            if os.path.exists(chroma_path):
                st.success("âœ… ChromaDB ë””ë ‰í† ë¦¬ ì¡´ì¬")
                
                # ChromaDB íŒŒì¼ ëª©ë¡
                try:
                    chroma_files = os.listdir(chroma_path)
                    if chroma_files:
                        st.write("ChromaDB íŒŒì¼:")
                        for file in chroma_files:
                            st.write(f"- {file}")
                    else:
                        st.warning("ChromaDBê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"ChromaDB íŒŒì¼ ëª©ë¡ í™•ì¸ ì‹¤íŒ¨: {e}")
            else:
                st.warning("âš ï¸ ChromaDB ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        with col2:
            st.subheader("ğŸ“‹ ëª¨ë¸ ì •ë³´")
            
            # ëª¨ë¸ ì •ë³´ íŒŒì¼ í™•ì¸
            model_info_path = get_model_info_path()
            if os.path.exists(model_info_path):
                st.success("âœ… ëª¨ë¸ ì •ë³´ íŒŒì¼ ì¡´ì¬")
                try:
                    with open(model_info_path, 'r', encoding='utf-8') as f:
                        model_info = json.load(f)
                        st.json(model_info)
                except Exception as e:
                    st.error(f"ëª¨ë¸ ì •ë³´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            else:
                st.warning("âš ï¸ ëª¨ë¸ ì •ë³´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # í˜„ì¬ ì„¸ì…˜ ìƒíƒœ ì •ë³´
            st.subheader("ğŸ›ï¸ í˜„ì¬ ì„¤ì •")
            st.info(f"ì„ íƒëœ AI ëª¨ë¸: {st.session_state.get('selected_model', 'llama3.2')}")
            st.info(f"ì„ íƒëœ ì„ë² ë”© ëª¨ë¸: {st.session_state.get('selected_embedding_model', 'sentence-transformers/all-mpnet-base-v2')}")
            st.info(f"ë²¡í„° DB ì´ˆê¸°í™”: {'âœ… ì™„ë£Œ' if st.session_state.get('vector_db_initialized', False) else 'âš ï¸ í•„ìš”'}")

if __name__ == "__main__":
    main() 